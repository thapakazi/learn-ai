{"/learn-ai/docs/":{"data":{"capstone-project-weeks-25-26#Capstone Project (Weeks 25-26)":"Build an end-to-end ML system incorporating:\nData pipeline Model training pipeline CI/CD integration Deployment to production Monitoring and alerting Documentation ","ml-engineering-syllabus-for-devopssre-professionals#ML Engineering Syllabus for DevOps/SRE Professionals":"","module-1-foundations-weeks-1-4#Module 1: Foundations (Weeks 1-4)":"","module-2-mlops-core-weeks-5-8#Module 2: MLOps Core (Weeks 5-8)":"","module-3-infrastructure--deployment-weeks-9-12#Module 3: Infrastructure \u0026amp; Deployment (Weeks 9-12)":"","module-4-data-engineering-for-ml-weeks-13-16#Module 4: Data Engineering for ML (Weeks 13-16)":"","module-5-monitoring--reliability-weeks-17-20#Module 5: Monitoring \u0026amp; Reliability (Weeks 17-20)":"ML Engineering Syllabus for DevOps/SRE ProfessionalsModule 1: Foundations (Weeks 1-4) Week 1: Python for ML Engineering Python Basics for DevOps/SRE NumPy for Infrastructure Metrics Pandas for Log Analysis Matplotlib for Monitoring Dashboards Week 2: Mathematics \u0026 Statistics Refresher Linear algebra essentials Probability and statistics Calculus for ML (gradients, optimization) Practical applications in ML Week 3: Machine Learning Fundamentals Supervised vs unsupervised learning Classification and regression Model evaluation metrics Overfitting and regularization Cross-validation techniques Week 4: Deep Learning Basics Neural network architecture Backpropagation and gradient descent Introduction to TensorFlow/PyTorch CNNs and RNNs overview Module 2: MLOps Core (Weeks 5-8) Week 5: Version Control for ML Data versioning with DVC Model versioning strategies Experiment tracking with MLflow/Weights \u0026 Biases Git workflows for ML projects Week 6: ML Pipeline Orchestration Apache Airflow for ML workflows Kubeflow Pipelines Prefect/Dagster alternatives Pipeline monitoring and alerting Week 7: CI/CD for ML Testing ML code and models Automated model validation Progressive deployment strategies A/B testing for models Shadow deployments Week 8: Model Registry \u0026 Governance Model registry patterns Model metadata management Compliance and audit trails Model approval workflows Module 3: Infrastructure \u0026 Deployment (Weeks 9-12) Week 9: Containerization for ML Docker for ML applications Multi-stage builds for optimization GPU support in containers Container registries for ML Week 10: Kubernetes for ML Kubernetes fundamentals review Kubeflow deployment GPU scheduling and management Auto-scaling ML workloads Service mesh for ML services Week 11: Model Serving REST vs gRPC for model serving TensorFlow Serving TorchServe ONNX Runtime Triton Inference Server Edge deployment considerations Week 12: Infrastructure as Code for ML Terraform for ML infrastructure Pulumi alternatives Cost optimization strategies Multi-cloud considerations Module 4: Data Engineering for ML (Weeks 13-16) Week 13: Data Pipeline Architecture Batch vs streaming data Apache Kafka for ML Apache Spark for preprocessing Data lake vs data warehouse Week 14: Feature Engineering \u0026 Stores Feature engineering best practices Feature stores (Feast, Tecton) Feature versioning Online vs offline features Week 15: Data Quality \u0026 Validation Data quality monitoring Schema validation Data drift detection Great Expectations framework Week 16: ETL/ELT for ML Building robust data pipelines Apache Beam DBT for ML Real-time feature computation Module 5: Monitoring \u0026 Reliability (Weeks 17-20) Week 17: Model Monitoring Performance metrics tracking Model drift detection Data drift vs concept drift Alerting strategies Week 18: Observability for ML Distributed tracing for ML Prometheus \u0026 Grafana for ML Custom metrics and dashboards Log aggregation patterns Week 19: ML System Reliability SLIs/SLOs/SLAs for ML systems Chaos engineering for ML Disaster recovery planning Rollback strategies Week 20: Performance Optimization Model optimization techniques Quantization and pruning Hardware acceleration (GPU/TPU) Caching strategies ","module-6-advanced-topics-weeks-21-24#Module 6: Advanced Topics (Weeks 21-24)":"Week 21: Distributed Training Data parallelism Model parallelism Horovod and distributed frameworks Cloud training platforms Week 22: AutoML \u0026 Hyperparameter Tuning Hyperparameter optimization AutoML platforms Neural Architecture Search Optuna/Ray Tune Week 23: LLMs in Production LLM deployment challenges Prompt engineering Fine-tuning strategies Vector databases RAG architectures Week 24: Security \u0026 Privacy Model security best practices Adversarial attacks and defenses Differential privacy Federated learning basics Compliance (GDPR, CCPA) ","recommended-resources#Recommended Resources":"Books ‚ÄúDesigning Machine Learning Systems‚Äù by Chip Huyen ‚ÄúMachine Learning Engineering‚Äù by Andriy Burkov ‚ÄúBuilding Machine Learning Powered Applications‚Äù by Emmanuel Ameisen ‚ÄúPractical MLOps‚Äù by Noah Gift \u0026 Alfredo Deza Online Courses Fast.ai Practical Deep Learning Andrew Ng‚Äôs Machine Learning Course Google Cloud ML Engineering Path AWS ML Specialty Certification Tools to Master Version Control: Git, DVC Orchestration: Airflow, Kubeflow Monitoring: Prometheus, Grafana, Evidently Deployment: Docker, Kubernetes, Helm Cloud: AWS SageMaker, GCP Vertex AI, Azure ML Frameworks: TensorFlow, PyTorch, Scikit-learn Hands-on Labs Set up a complete MLOps pipeline Deploy a model with canary releases Implement feature store Build a model monitoring dashboard Create a data validation pipeline "},"title":"Complete Syllabus"},"/learn-ai/docs/fundamentals/":{"data":{"assessment-checkpoints#Assessment Checkpoints":"Week 1-2 Checkpoint Python coding assessment Mathematical problem set Week 3-4 Project Build an ML pipeline that:\nIngests operational data Trains a model Evaluates performance Provides predictions via API ","fundamentals-module#Fundamentals Module":"Fundamentals ModuleThis module establishes the foundational knowledge required for ML Engineering, building upon your existing DevOps/SRE expertise.","learning-objectives#Learning Objectives":"By the end of this module, you will:\nMaster Python for ML engineering tasks Understand core mathematical concepts used in ML Grasp fundamental ML algorithms and their applications Get hands-on experience with deep learning frameworks ","module-structure#Module Structure":" Week 1: Python FundamentalsPython essentials with DevOps/SRE focus Week 2: Mathematics \u0026 StatisticsCore math concepts for ML (Coming Soon) Week 3: ML FundamentalsMachine learning algorithms (Coming Soon) Week 4: Deep LearningNeural networks basics (Coming Soon) ","next-steps#Next Steps":"After completing this module, you‚Äôll be ready for Module 2: MLOps Core, where you‚Äôll apply DevOps principles to machine learning workflows.","quick-navigation#Quick Navigation":"üìö Week 1: Python for ML Engineering Python Basics for DevOps/SRE NumPy for Infrastructure Metrics Pandas for Log Analysis Matplotlib for Monitoring Dashboards üìê Week 2: Mathematics \u0026 Statistics Content coming soon\nLinear algebra fundamentals Statistics for ML Calculus essentials ü§ñ Week 3: Machine Learning Fundamentals Content coming soon\nSupervised learning algorithms Unsupervised learning techniques Model evaluation metrics üß† Week 4: Deep Learning Basics Content coming soon\nNeural network architectures TensorFlow/PyTorch introduction CNNs, RNNs, and Transformers ","resources#Resources":"Required Reading ‚ÄúPython Machine Learning‚Äù by Sebastian Raschka (Chapters 1-4) ‚ÄúThe Elements of Statistical Learning‚Äù (Selected sections) Online Materials Fast.ai Practical Deep Learning (Lessons 1-3) 3Blue1Brown Neural Network series Google‚Äôs Machine Learning Crash Course Andrew Ng‚Äôs Machine Learning Course ","tools-setup#Tools Setup":" # Create virtual environment python -m venv ml-env source ml-env/bin/activate # Install core packages pip install numpy pandas scikit-learn pip install tensorflow pytorch pip install jupyter mlflow "},"title":"Module 1: Fundamentals"},"/learn-ai/docs/fundamentals/week1/":{"data":{"":"","additional-resources#Additional Resources":"Books Python for DevOps (O‚ÄôReilly) Automate the Boring Stuff with Python Python Data Science Handbook Online Courses Real Python - Python for DevOps DataCamp - Data Manipulation with Python Coursera - Python for Everybody Documentation Python Official Documentation NumPy Documentation Pandas Documentation Matplotlib Documentation ","assessment-checklist#Assessment Checklist":"‚úÖ Python Basics\nCan write functions with error handling Understand file I/O operations Can work with JSON/YAML configurations Able to create reusable modules ‚úÖ NumPy\nCan create and manipulate arrays Understand array broadcasting Can perform statistical operations Able to optimize performance with vectorization ‚úÖ Pandas\nCan load and parse various data formats Understand DataFrame operations Can perform time series analysis Able to aggregate and group data ‚úÖ Matplotlib\nCan create basic plots Understand subplot layouts Can customize visualizations Able to create dashboard-style reports ","hands-on-projects#Hands-On Projects":"Project 1: Automated Health Check System Build a complete health check system that:\nMonitors multiple services (Python basics) Collects and processes metrics (NumPy) Analyzes historical data for trends (Pandas) Generates visual reports (Matplotlib) Project 2: Log Analysis Pipeline Create an end-to-end log analysis pipeline that:\nParses application logs Detects anomalies and patterns Generates daily/weekly reports Visualizes error trends Project 3: Capacity Planning Tool Develop a capacity planning tool that:\nCollects resource utilization data Predicts future resource needs Identifies optimization opportunities Creates executive dashboards ","learning-objectives#Learning Objectives":"By the end of this week, you will be able to:\nWrite Python scripts for infrastructure automation and monitoring Process and analyze large-scale metrics data efficiently with NumPy Perform log analysis and create reports using Pandas Create professional monitoring dashboards and visualizations with Matplotlib ","learning-path#Learning Path":"Day 1-2: Python Basics Review Python fundamentals Complete automation exercises Build your first monitoring script Day 3: NumPy Learn array operations Practice with metrics data Implement anomaly detection Day 4: Pandas Master DataFrame operations Analyze sample log files Create incident reports Day 5: Matplotlib Learn visualization basics Build monitoring dashboards Create professional reports Day 6-7: Integration Project Combine all skills Build a complete monitoring solution Document and test your code ","next-steps#Next Steps":"After completing Week 1, you‚Äôll be ready to move on to:\nWeek 2: Linear Algebra and Statistics for ML Week 3: Introduction to Machine Learning Week 4: Deep Learning Fundamentals Remember: The goal is not just to learn Python, but to apply it effectively in DevOps/SRE contexts. Focus on building practical, production-ready solutions!","overview#Overview":"This week covers the essential Python foundations needed for machine learning in DevOps and SRE contexts. Each topic is presented with real-world infrastructure and operations examples to make the concepts immediately applicable to your daily work.","prerequisites#Prerequisites":" Basic command line familiarity Understanding of basic DevOps concepts Python 3.8+ installed Access to a development environment ","setup-instructions#Setup Instructions":" # Create a virtual environment python3 -m venv ml-devops-env source ml-devops-env/bin/activate # On Windows: ml-devops-env\\Scripts\\activate # Install required packages pip install numpy pandas matplotlib requests psutil pyyaml # Verify installation python -c \"import numpy, pandas, matplotlib; print('All packages installed successfully!')\" ","topics-covered#Topics Covered":" Python BasicsCore Python concepts with DevOps applications NumPy ArraysEfficient numerical operations for infrastructure metrics Pandas DataFramesLog analysis and structured data management Matplotlib VisualizationCreating monitoring dashboards and reports "},"title":"Week 1: Python Fundamentals"},"/learn-ai/docs/fundamentals/week1/matplotlib-visualization/":{"data":{"":"","core-concepts-with-devops-applications#Core Concepts with DevOps Applications":"1. Basic Monitoring Dashboards DevOps Context: Real-time system metrics visualization\nimport matplotlib.pyplot as plt import numpy as np import pandas as pd from datetime import datetime, timedelta import matplotlib.dates as mdates from matplotlib.gridspec import GridSpec # Generate sample monitoring data def generate_monitoring_data(): \"\"\"Generate realistic monitoring metrics\"\"\" hours = 24 time_points = pd.date_range(start='2024-01-15', periods=hours*60, freq='1min') # CPU usage with spikes cpu = 40 + 10 * np.sin(np.linspace(0, 4*np.pi, len(time_points))) cpu += np.random.normal(0, 5, len(time_points)) cpu[300:320] = 85 + np.random.normal(0, 5, 20) # Spike # Memory usage with gradual increase memory = 50 + np.linspace(0, 20, len(time_points)) memory += np.random.normal(0, 3, len(time_points)) # Network I/O network_in = np.random.exponential(50, len(time_points)) network_out = np.random.exponential(30, len(time_points)) # Disk I/O disk_read = np.random.exponential(20, len(time_points)) disk_write = np.random.exponential(15, len(time_points)) return pd.DataFrame({ 'timestamp': time_points, 'cpu': np.clip(cpu, 0, 100), 'memory': np.clip(memory, 0, 100), 'network_in': network_in, 'network_out': network_out, 'disk_read': disk_read, 'disk_write': disk_write }) # Create monitoring dashboard def create_monitoring_dashboard(df): \"\"\"Create a comprehensive monitoring dashboard\"\"\" fig = plt.figure(figsize=(16, 10)) fig.suptitle('System Monitoring Dashboard - Production Server', fontsize=16, fontweight='bold') gs = GridSpec(3, 2, figure=fig, hspace=0.3, wspace=0.3) # CPU Usage ax1 = fig.add_subplot(gs[0, 0]) ax1.plot(df['timestamp'], df['cpu'], color='#FF6B6B', linewidth=1.5) ax1.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='Warning (80%)') ax1.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='Critical (90%)') ax1.fill_between(df['timestamp'], df['cpu'], alpha=0.3, color='#FF6B6B') ax1.set_title('CPU Usage (%)', fontweight='bold') ax1.set_ylabel('Usage (%)') ax1.set_ylim(0, 100) ax1.grid(True, alpha=0.3) ax1.legend(loc='upper right') # Memory Usage ax2 = fig.add_subplot(gs[0, 1]) ax2.plot(df['timestamp'], df['memory'], color='#4ECDC4', linewidth=1.5) ax2.axhline(y=85, color='orange', linestyle='--', alpha=0.7, label='Warning (85%)') ax2.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='Critical (95%)') ax2.fill_between(df['timestamp'], df['memory'], alpha=0.3, color='#4ECDC4') ax2.set_title('Memory Usage (%)', fontweight='bold') ax2.set_ylabel('Usage (%)') ax2.set_ylim(0, 100) ax2.grid(True, alpha=0.3) ax2.legend(loc='upper right') # Network I/O ax3 = fig.add_subplot(gs[1, 0]) ax3.plot(df['timestamp'], df['network_in'], label='Inbound', color='#45B7D1', linewidth=1) ax3.plot(df['timestamp'], df['network_out'], label='Outbound', color='#FFA07A', linewidth=1) ax3.set_title('Network I/O (MB/s)', fontweight='bold') ax3.set_ylabel('Throughput (MB/s)') ax3.grid(True, alpha=0.3) ax3.legend() # Disk I/O ax4 = fig.add_subplot(gs[1, 1]) ax4.plot(df['timestamp'], df['disk_read'], label='Read', color='#98D8C8', linewidth=1) ax4.plot(df['timestamp'], df['disk_write'], label='Write', color='#F7DC6F', linewidth=1) ax4.set_title('Disk I/O (MB/s)', fontweight='bold') ax4.set_ylabel('Throughput (MB/s)') ax4.grid(True, alpha=0.3) ax4.legend() # Combined System Health Score ax5 = fig.add_subplot(gs[2, :]) # Calculate health score (inverse of resource usage) health_score = 100 - (df['cpu'] * 0.4 + df['memory'] * 0.4 + np.clip(df['network_in'] + df['network_out'], 0, 100) * 0.1 + np.clip(df['disk_read'] + df['disk_write'], 0, 100) * 0.1) ax5.plot(df['timestamp'], health_score, color='#2ECC71', linewidth=2) ax5.fill_between(df['timestamp'], health_score, alpha=0.3, color='#2ECC71') ax5.axhline(y=30, color='red', linestyle='--', alpha=0.7, label='Poor Health (\u003c30)') ax5.axhline(y=60, color='orange', linestyle='--', alpha=0.7, label='Fair Health (30-60)') ax5.set_title('System Health Score', fontweight='bold') ax5.set_ylabel('Health Score') ax5.set_xlabel('Time') ax5.set_ylim(0, 100) ax5.grid(True, alpha=0.3) ax5.legend() # Format x-axis for all subplots for ax in [ax1, ax2, ax3, ax4, ax5]: ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M')) ax.xaxis.set_major_locator(mdates.HourLocator(interval=4)) plt.setp(ax.xaxis.get_majorticklabels(), rotation=45) plt.tight_layout() return fig # Generate and visualize data df_monitoring = generate_monitoring_data() dashboard = create_monitoring_dashboard(df_monitoring) plt.show() 2. SLA and Performance Reports DevOps Context: Service Level Agreement compliance visualization\ndef create_sla_report(service_name=\"API Gateway\"): \"\"\"Create SLA compliance visualization report\"\"\" # Generate sample SLA data days = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D') # Daily metrics availability = np.random.normal(99.5, 0.5, len(days)) availability[5] = 98.2 # Incident day availability[15] = 97.8 # Another incident availability = np.clip(availability, 95, 100) response_time_p99 = np.random.normal(200, 30, len(days)) response_time_p99[5] = 450 # Spike response_time_p99[15] = 380 error_rate = np.random.exponential(0.5, len(days)) error_rate[5] = 3.2 error_rate[15] = 2.8 # Create figure with subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) fig.suptitle(f'SLA Compliance Report - {service_name} (January 2024)', fontsize=16, fontweight='bold') # 1. Availability Trend ax1 = axes[0, 0] ax1.plot(days, availability, marker='o', color='#2ECC71', linewidth=2, markersize=4) ax1.axhline(y=99.9, color='gold', linestyle='--', label='SLA Target (99.9%)') ax1.fill_between(days, availability, 99.9, where=(availability \u003e= 99.9), color='green', alpha=0.3, label='Above SLA') ax1.fill_between(days, availability, 99.9, where=(availability \u003c 99.9), color='red', alpha=0.3, label='Below SLA') ax1.set_title('Daily Availability (%)', fontweight='bold') ax1.set_ylabel('Availability (%)') ax1.set_ylim(97, 100) ax1.grid(True, alpha=0.3) ax1.legend() # 2. Response Time P99 ax2 = axes[0, 1] bars = ax2.bar(days, response_time_p99, color=['red' if x \u003e 300 else 'green' for x in response_time_p99], alpha=0.7) ax2.axhline(y=300, color='red', linestyle='--', label='SLA Limit (300ms)') ax2.set_title('P99 Response Time (ms)', fontweight='bold') ax2.set_ylabel('Response Time (ms)') ax2.legend() ax2.grid(True, alpha=0.3, axis='y') # 3. Error Rate Distribution ax3 = axes[1, 0] # Create box plot for weekly error rates weekly_errors = [error_rate[i:i+7] for i in range(0, len(error_rate), 7)] week_labels = [f'Week {i+1}' for i in range(len(weekly_errors))] bp = ax3.boxplot(weekly_errors, labels=week_labels, patch_artist=True) for patch in bp['boxes']: patch.set_facecolor('#FF6B6B') patch.set_alpha(0.7) ax3.axhline(y=1, color='orange', linestyle='--', label='Warning (1%)') ax3.axhline(y=2, color='red', linestyle='--', label='Critical (2%)') ax3.set_title('Weekly Error Rate Distribution (%)', fontweight='bold') ax3.set_ylabel('Error Rate (%)') ax3.legend() ax3.grid(True, alpha=0.3, axis='y') # 4. SLA Summary Gauge ax4 = axes[1, 1] ax4.axis('off') # Calculate monthly SLA compliance monthly_availability = np.mean(availability) sla_violations = np.sum(response_time_p99 \u003e 300) avg_error_rate = np.mean(error_rate) # Create text summary summary_text = f\"\"\" Monthly SLA Summary {'='*30} Availability: {monthly_availability:.2f}% Status: {'‚úÖ PASS' if monthly_availability \u003e= 99.9 else '‚ùå FAIL'} P99 Response Time Violations: {sla_violations} days Status: {'‚úÖ PASS' if sla_violations \u003c= 2 else '‚ö†Ô∏è WARNING' if sla_violations \u003c= 5 else '‚ùå FAIL'} Average Error Rate: {avg_error_rate:.2f}% Status: {'‚úÖ PASS' if avg_error_rate \u003c 1 else '‚ö†Ô∏è WARNING' if avg_error_rate \u003c 2 else '‚ùå FAIL'} Overall Compliance: {'‚úÖ COMPLIANT' if monthly_availability \u003e= 99.9 and sla_violations \u003c= 2 and avg_error_rate \u003c 1 else '‚ùå NON-COMPLIANT'} \"\"\" ax4.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=11, fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)) # Format dates on x-axis for ax in [ax1, ax2]: ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) ax.xaxis.set_major_locator(mdates.DayLocator(interval=5)) plt.setp(ax.xaxis.get_majorticklabels(), rotation=45) plt.tight_layout() return fig # Create SLA report sla_report = create_sla_report() plt.show() 3. Capacity Planning Visualizations DevOps Context: Resource utilization trends and forecasting\ndef create_capacity_planning_viz(): \"\"\"Visualize capacity trends and projections\"\"\" # Historical data (90 days) historical_days = pd.date_range(start='2023-11-01', end='2024-01-31', freq='D') # Storage growth pattern storage_base = 500 # GB storage_growth = np.linspace(0, 300, len(historical_days)) storage_noise = np.random.normal(0, 20, len(historical_days)) storage_used = storage_base + storage_growth + storage_noise # CPU usage pattern (weekly cycles) cpu_pattern = 40 + 20 * np.sin(np.linspace(0, 26*np.pi, len(historical_days))) cpu_noise = np.random.normal(0, 5, len(historical_days)) cpu_used = cpu_pattern + cpu_noise # Future projection (30 days) future_days = pd.date_range(start='2024-02-01', end='2024-03-01', freq='D') # Linear regression for storage projection from sklearn.linear_model import LinearRegression X = np.arange(len(historical_days)).reshape(-1, 1) model = LinearRegression() model.fit(X, storage_used) X_future = np.arange(len(historical_days), len(historical_days) + len(future_days)).reshape(-1, 1) storage_projected = model.predict(X_future) # Create visualization fig, axes = plt.subplots(2, 2, figsize=(15, 10)) fig.suptitle('Capacity Planning Dashboard', fontsize=16, fontweight='bold') # 1. Storage Capacity Trend ax1 = axes[0, 0] ax1.plot(historical_days, storage_used, color='#3498DB', linewidth=2, label='Actual Usage') ax1.plot(future_days, storage_projected, color='#E74C3C', linewidth=2, linestyle='--', label='Projected Usage') ax1.axhline(y=1000, color='red', linestyle='-', linewidth=2, label='Current Capacity (1TB)') ax1.fill_between(historical_days, storage_used, alpha=0.3, color='#3498DB') ax1.fill_between(future_days, storage_projected, alpha=0.3, color='#E74C3C') # Mark when capacity will be exceeded exceed_date = None for i, val in enumerate(storage_projected): if val \u003e 1000: exceed_date = future_days[i] ax1.axvline(x=exceed_date, color='red', linestyle=':', alpha=0.7) ax1.text(exceed_date, 1050, f'Capacity Exceeded\\n{exceed_date.strftime(\"%Y-%m-%d\")}', ha='center', fontsize=9, color='red') break ax1.set_title('Storage Capacity Planning', fontweight='bold') ax1.set_ylabel('Storage (GB)') ax1.legend() ax1.grid(True, alpha=0.3) # 2. CPU Usage Heatmap (by hour and day of week) ax2 = axes[0, 1] # Generate hourly CPU data hours = 24 days_of_week = 7 cpu_heatmap_data = np.random.normal(50, 15, (hours, days_of_week)) # Business hours have higher usage for day in range(5): # Monday to Friday for hour in range(9, 18): # 9 AM to 6 PM cpu_heatmap_data[hour, day] += 20 im = ax2.imshow(cpu_heatmap_data, cmap='YlOrRd', aspect='auto', vmin=0, vmax=100) ax2.set_title('CPU Usage Heatmap (Weekly Pattern)', fontweight='bold') ax2.set_xlabel('Day of Week') ax2.set_ylabel('Hour of Day') ax2.set_xticks(range(7)) ax2.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']) ax2.set_yticks(range(0, 24, 3)) ax2.set_yticklabels([f'{h:02d}:00' for h in range(0, 24, 3)]) # Add colorbar cbar = plt.colorbar(im, ax=ax2) cbar.set_label('CPU Usage (%)', rotation=270, labelpad=15) # 3. Resource Utilization Comparison ax3 = axes[1, 0] resources = ['CPU', 'Memory', 'Storage', 'Network', 'GPU'] current_usage = [65, 78, 82, 45, 92] projected_usage = [72, 85, 95, 52, 98] x = np.arange(len(resources)) width = 0.35 bars1 = ax3.bar(x - width/2, current_usage, width, label='Current', color='#2ECC71') bars2 = ax3.bar(x + width/2, projected_usage, width, label='Projected (30 days)', color='#E67E22') # Add capacity line ax3.axhline(y=85, color='red', linestyle='--', label='Recommended Max (85%)') # Add value labels on bars for bars in [bars1, bars2]: for bar in bars: height = bar.get_height() ax3.text(bar.get_x() + bar.get_width()/2., height, f'{height:.0f}%', ha='center', va='bottom', fontsize=9) ax3.set_title('Resource Utilization Overview', fontweight='bold') ax3.set_ylabel('Utilization (%)') ax3.set_xticks(x) ax3.set_xticklabels(resources) ax3.legend() ax3.set_ylim(0, 110) ax3.grid(True, alpha=0.3, axis='y') # 4. Growth Rate Analysis ax4 = axes[1, 1] # Calculate growth rates metrics = ['Users', 'Requests/day', 'Storage', 'Bandwidth'] growth_rates = [12.5, 18.3, 22.1, 15.7] # Monthly growth % # Create circular progress indicators theta = np.linspace(0, 2*np.pi, 100) for i, (metric, rate) in enumerate(zip(metrics, growth_rates)): # Position for each gauge cx = 0.25 + (i % 2) * 0.5 cy = 0.65 - (i // 2) * 0.5 # Draw gauge background ax4.plot(cx + 0.15*np.cos(theta), cy + 0.15*np.sin(theta), 'lightgray', linewidth=10) # Draw gauge progress progress_theta = theta[:int(rate/30 * 100)] color = 'green' if rate \u003c 15 else 'orange' if rate \u003c 25 else 'red' ax4.plot(cx + 0.15*np.cos(progress_theta), cy + 0.15*np.sin(progress_theta), color=color, linewidth=10) # Add text ax4.text(cx, cy, f'{rate:.1f}%', ha='center', va='center', fontweight='bold') ax4.text(cx, cy-0.25, metric, ha='center', va='center', fontsize=9) ax4.set_title('Monthly Growth Rates', fontweight='bold') ax4.set_xlim(0, 1) ax4.set_ylim(0, 1) ax4.axis('off') # Format date axes for ax in [ax1]: ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m')) ax.xaxis.set_major_locator(mdates.MonthLocator()) plt.setp(ax.xaxis.get_majorticklabels(), rotation=45) plt.tight_layout() return fig # Create capacity planning visualization capacity_viz = create_capacity_planning_viz() plt.show() 4. Incident and Alert Visualizations DevOps Context: Incident patterns and alert frequency analysis\ndef create_incident_analytics(): \"\"\"Create incident analysis visualizations\"\"\" # Generate incident data np.random.seed(42) # Incident times (more during business hours) incident_times = [] for day in range(30): # Business hours incidents n_business = np.random.poisson(3) for _ in range(n_business): hour = np.random.uniform(9, 18) incident_times.append(day * 24 + hour) # Off-hours incidents n_offhours = np.random.poisson(1) for _ in range(n_offhours): hour = np.random.choice([np.random.uniform(0, 9), np.random.uniform(18, 24)]) incident_times.append(day * 24 + hour) # Create figure fig = plt.figure(figsize=(16, 10)) gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3) # 1. Incident Timeline ax1 = fig.add_subplot(gs[0, :]) # Convert to datetime base_date = datetime(2024, 1, 1) incident_datetimes = [base_date + timedelta(hours=h) for h in incident_times] # Plot incidents as scatter severities = np.random.choice(['P1', 'P2', 'P3'], len(incident_datetimes), p=[0.1, 0.3, 0.6]) colors = {'P1': 'red', 'P2': 'orange', 'P3': 'yellow'} for severity in ['P1', 'P2', 'P3']: mask = [s == severity for s in severities] times = [t for t, m in zip(incident_datetimes, mask) if m] y_values = [1] * len(times) ax1.scatter(times, y_values, c=colors[severity], s=100, alpha=0.7, label=severity) ax1.set_title('Incident Timeline (30 Days)', fontweight='bold') ax1.set_xlabel('Date') ax1.set_ylim(0.5, 1.5) ax1.set_yticks([]) ax1.legend() ax1.grid(True, alpha=0.3, axis='x') ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) # 2. Incidents by Hour of Day ax2 = fig.add_subplot(gs[1, 0]) hours = [t.hour for t in incident_datetimes] ax2.hist(hours, bins=24, color='#3498DB', alpha=0.7, edgecolor='black') ax2.axvspan(9, 18, alpha=0.2, color='yellow', label='Business Hours') ax2.set_title('Incidents by Hour of Day', fontweight='bold') ax2.set_xlabel('Hour') ax2.set_ylabel('Count') ax2.set_xticks(range(0, 24, 3)) ax2.legend() ax2.grid(True, alpha=0.3, axis='y') # 3. Incidents by Day of Week ax3 = fig.add_subplot(gs[1, 1]) days = [t.strftime('%A') for t in incident_datetimes] day_counts = pd.Series(days).value_counts() day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] day_counts = day_counts.reindex(day_order, fill_value=0) bars = ax3.bar(range(7), day_counts.values, color=['#E74C3C' if d in ['Saturday', 'Sunday'] else '#3498DB' for d in day_order]) ax3.set_title('Incidents by Day of Week', fontweight='bold') ax3.set_xlabel('Day') ax3.set_ylabel('Count') ax3.set_xticks(range(7)) ax3.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']) ax3.grid(True, alpha=0.3, axis='y') # 4. Service Impact Analysis ax4 = fig.add_subplot(gs[1, 2]) services = ['API', 'Database', 'Auth', 'Cache', 'Queue'] impacts = [15, 8, 12, 5, 3] # Create pie chart colors_pie = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'] wedges, texts, autotexts = ax4.pie(impacts, labels=services, colors=colors_pie, autopct='%1.1f%%', startangle=90) ax4.set_title('Incidents by Service', fontweight='bold') # 5. MTTR Trend ax5 = fig.add_subplot(gs[2, 0]) weeks = ['Week 1', 'Week 2', 'Week 3', 'Week 4'] mttr_p1 = [45, 38, 42, 35] mttr_p2 = [120, 110, 95, 88] mttr_p3 = [180, 165, 150, 140] x = np.arange(len(weeks)) width = 0.25 ax5.bar(x - width, mttr_p1, width, label='P1', color='red', alpha=0.7) ax5.bar(x, mttr_p2, width, label='P2', color='orange', alpha=0.7) ax5.bar(x + width, mttr_p3, width, label='P3', color='yellow', alpha=0.7) ax5.set_title('Mean Time to Resolution (MTTR) Trend', fontweight='bold') ax5.set_xlabel('Week') ax5.set_ylabel('MTTR (minutes)') ax5.set_xticks(x) ax5.set_xticklabels(weeks) ax5.legend() ax5.grid(True, alpha=0.3, axis='y') # 6. Alert Noise Analysis ax6 = fig.add_subplot(gs[2, 1:]) # Generate alert data hours_range = pd.date_range(start='2024-01-01', periods=168, freq='1H') # 1 week alerts = np.random.poisson(5, len(hours_range)) actionable = np.random.binomial(alerts, 0.3) # 30% actionable ax6.fill_between(hours_range, alerts, alpha=0.3, color='red', label='Total Alerts') ax6.fill_between(hours_range, actionable, alpha=0.5, color='green', label='Actionable Alerts') ax6.set_title('Alert Noise Analysis (1 Week)', fontweight='bold') ax6.set_xlabel('Date') ax6.set_ylabel('Alert Count') ax6.legend() ax6.grid(True, alpha=0.3) ax6.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) # Add noise ratio text noise_ratio = (1 - np.sum(actionable) / np.sum(alerts)) * 100 ax6.text(0.02, 0.95, f'Noise Ratio: {noise_ratio:.1f}%', transform=ax6.transAxes, fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)) plt.tight_layout() return fig # Create incident analytics incident_viz = create_incident_analytics() plt.show() 5. Deployment and Release Visualizations DevOps Context: Deployment frequency and success rates\ndef create_deployment_dashboard(): \"\"\"Create deployment analytics dashboard\"\"\" fig, axes = plt.subplots(2, 3, figsize=(15, 10)) fig.suptitle('Deployment Analytics Dashboard', fontsize=16, fontweight='bold') # 1. Deployment Frequency Over Time ax1 = axes[0, 0] dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D') deployments_per_day = np.random.poisson(3, len(dates)) deployments_per_day[5] = 0 # Weekend deployments_per_day[6] = 0 deployments_per_day[12] = 0 deployments_per_day[13] = 0 ax1.bar(dates, deployments_per_day, color='#3498DB', alpha=0.7) ax1.axhline(y=np.mean(deployments_per_day), color='red', linestyle='--', label=f'Average: {np.mean(deployments_per_day):.1f}') ax1.set_title('Daily Deployment Frequency', fontweight='bold') ax1.set_ylabel('Number of Deployments') ax1.legend() ax1.grid(True, alpha=0.3, axis='y') ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45) # 2. Success Rate by Environment ax2 = axes[0, 1] environments = ['Dev', 'Staging', 'Production'] success_rates = [95, 92, 98] colors_env = ['#2ECC71', '#F39C12', '#E74C3C'] bars = ax2.bar(environments, success_rates, color=colors_env, alpha=0.7) ax2.axhline(y=95, color='red', linestyle='--', label='Target: 95%') ax2.set_title('Deployment Success Rate by Environment', fontweight='bold') ax2.set_ylabel('Success Rate (%)') ax2.set_ylim(85, 100) ax2.legend() ax2.grid(True, alpha=0.3, axis='y') # Add value labels for bar, rate in zip(bars, success_rates): ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{rate}%', ha='center', fontweight='bold') # 3. Deployment Duration Distribution ax3 = axes[0, 2] durations = np.concatenate([ np.random.normal(5, 1, 50), # Quick deployments np.random.normal(15, 3, 30), # Normal deployments np.random.normal(30, 5, 10) # Long deployments ]) ax3.hist(durations, bins=20, color='#9B59B6', alpha=0.7, edgecolor='black') ax3.axvline(x=np.median(durations), color='red', linestyle='--', label=f'Median: {np.median(durations):.1f} min') ax3.set_title('Deployment Duration Distribution', fontweight='bold') ax3.set_xlabel('Duration (minutes)') ax3.set_ylabel('Frequency') ax3.legend() ax3.grid(True, alpha=0.3, axis='y') # 4. Rollback Analysis ax4 = axes[1, 0] services = ['API', 'Frontend', 'Backend', 'Database', 'Cache'] rollback_counts = [2, 5, 3, 1, 0] total_deployments = [45, 52, 38, 15, 20] rollback_rates = [r/t*100 for r, t in zip(rollback_counts, total_deployments)] y_pos = np.arange(len(services)) bars = ax4.barh(y_pos, rollback_rates, color=['red' if r \u003e 5 else 'orange' if r \u003e 2 else 'green' for r in rollback_rates]) ax4.set_yticks(y_pos) ax4.set_yticklabels(services) ax4.set_title('Rollback Rate by Service', fontweight='bold') ax4.set_xlabel('Rollback Rate (%)') ax4.grid(True, alpha=0.3, axis='x') # Add value labels for i, (bar, rate) in enumerate(zip(bars, rollback_rates)): ax4.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, f'{rate:.1f}%', va='center') # 5. Deployment Methods ax5 = axes[1, 1] methods = ['Blue-Green', 'Canary', 'Rolling', 'Recreate'] method_counts = [35, 25, 30, 10] colors_methods = ['#3498DB', '#E67E22', '#2ECC71', '#E74C3C'] wedges, texts, autotexts = ax5.pie(method_counts, labels=methods, colors=colors_methods, autopct='%1.1f%%', startangle=90) ax5.set_title('Deployment Methods Used', fontweight='bold') # 6. Lead Time Trend ax6 = axes[1, 2] weeks = pd.date_range(start='2024-01-01', periods=12, freq='W') lead_time = [5, 4.5, 4.2, 3.8, 3.5, 3.2, 3.0, 2.8, 2.7, 2.5, 2.4, 2.3] ax6.plot(weeks, lead_time, marker='o', color='#16A085', linewidth=2, markersize=8) ax6.fill_between(weeks, lead_time, alpha=0.3, color='#16A085') ax6.set_title('Lead Time Trend (Commit to Production)', fontweight='bold') ax6.set_ylabel('Lead Time (days)') ax6.grid(True, alpha=0.3) ax6.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45) # Add trend annotation ax6.annotate(f'Improvement: {(lead_time[0] - lead_time[-1])/lead_time[0]*100:.1f}%', xy=(weeks[-1], lead_time[-1]), xytext=(weeks[-3], lead_time[-1] + 1), arrowprops=dict(arrowstyle='-\u003e', color='green'), fontsize=10, color='green', fontweight='bold') plt.tight_layout() return fig # Create deployment dashboard deployment_viz = create_deployment_dashboard() plt.show() ","overview#Overview":"Matplotlib is essential for creating monitoring dashboards, performance reports, and infrastructure visualizations in DevOps/SRE. It transforms metrics and logs into actionable insights through clear, professional visualizations.","practice-exercises#Practice Exercises":" Custom Alerting Dashboard: Create a dashboard that shows alert fatigue metrics, including false positive rates and alert response times.\nDatabase Performance Visualization: Build visualizations for database metrics including query performance, connection pools, and slow query analysis.\nNetwork Traffic Flow: Create a Sankey diagram or flow visualization showing traffic between services.\nCI/CD Pipeline Metrics: Visualize build times, test coverage trends, and pipeline success rates.","real-world-devops-examples#Real-World DevOps Examples":"Example 1: Multi-Service Performance Comparison def create_service_comparison_dashboard(): \"\"\"Compare performance metrics across multiple services\"\"\" services = ['Auth API', 'User API', 'Order API', 'Payment API', 'Notification API'] # Generate metrics for each service metrics = { 'Availability (%)': [99.95, 99.88, 99.92, 99.99, 99.85], 'Avg Response (ms)': [45, 120, 85, 200, 35], 'Error Rate (%)': [0.5, 1.2, 0.8, 0.1, 1.5], 'Requests/sec': [500, 1200, 800, 300, 2000], 'CPU Usage (%)': [35, 65, 55, 45, 70] } fig, ax = plt.subplots(figsize=(12, 8)) # Normalize metrics for radar chart from math import pi categories = list(metrics.keys()) N = len(categories) # Create angles for radar chart angles = [n / float(N) * 2 * pi for n in range(N)] angles += angles[:1] # Initialize the plot ax = plt.subplot(111, projection='polar') # Define colors for each service colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'] for idx, service in enumerate(services): values = [] for metric in metrics.keys(): # Normalize values (0-100 scale) if 'Rate' in metric or 'Error' in metric: # Invert error metrics (lower is better) normalized = 100 - (metrics[metric][idx] / max(metrics[metric]) * 100) else: normalized = metrics[metric][idx] / max(metrics[metric]) * 100 values.append(normalized) values += values[:1] # Plot ax.plot(angles, values, 'o-', linewidth=2, label=service, color=colors[idx]) ax.fill(angles, values, alpha=0.25, color=colors[idx]) # Fix axis labels ax.set_xticks(angles[:-1]) ax.set_xticklabels(categories) ax.set_ylim(0, 100) # Add legend and title plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0)) plt.title('Service Performance Comparison (Normalized)', size=14, fontweight='bold', pad=20) return fig # Create service comparison service_comparison = create_service_comparison_dashboard() plt.show() Example 2: Cost Optimization Opportunities def visualize_cost_optimization(): \"\"\"Visualize cloud cost optimization opportunities\"\"\" fig, axes = plt.subplots(2, 2, figsize=(14, 10)) fig.suptitle('Cloud Cost Optimization Analysis', fontsize=16, fontweight='bold') # 1. Unused Resources ax1 = axes[0, 0] resource_types = ['Unattached\\nEBS', 'Idle\\nLoad Balancers', 'Unused\\nElastic IPs', 'Old\\nSnapshots', 'Stopped\\nInstances'] monthly_costs = [450, 280, 120, 350, 890] bars = ax1.bar(resource_types, monthly_costs, color='#E74C3C', alpha=0.7) ax1.set_title('Unused Resources - Monthly Cost', fontweight='bold') ax1.set_ylabel('Cost ($)') ax1.grid(True, alpha=0.3, axis='y') # Add total savings annotation total_savings = sum(monthly_costs) ax1.text(0.5, 0.95, f'Total Potential Savings: ${total_savings:,.0f}/month', transform=ax1.transAxes, ha='center', fontsize=11, fontweight='bold', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3)) # 2. Right-sizing Opportunities ax2 = axes[0, 1] instance_types = ['Over-provisioned', 'Right-sized', 'Under-provisioned'] instance_counts = [45, 120, 15] colors_sizing = ['#E74C3C', '#2ECC71', '#F39C12'] wedges, texts, autotexts = ax2.pie(instance_counts, labels=instance_types, colors=colors_sizing, autopct='%1.1f%%', startangle=90) ax2.set_title('EC2 Instance Sizing Analysis', fontweight='bold') # 3. Reserved vs On-Demand ax3 = axes[1, 0] months = pd.date_range(start='2024-01', periods=6, freq='M') on_demand_costs = [12000, 13000, 14000, 15000, 16000, 17000] reserved_costs = [8000, 8000, 8000, 8000, 8000, 8000] ax3.plot(months, on_demand_costs, marker='o', label='On-Demand', color='#E74C3C', linewidth=2) ax3.plot(months, reserved_costs, marker='s', label='Reserved (1-year)', color='#2ECC71', linewidth=2) ax3.fill_between(months, on_demand_costs, reserved_costs, alpha=0.3, color='yellow', label='Potential Savings') ax3.set_title('Reserved vs On-Demand Cost Comparison', fontweight='bold') ax3.set_ylabel('Monthly Cost ($)') ax3.legend() ax3.grid(True, alpha=0.3) ax3.xaxis.set_major_formatter(mdates.DateFormatter('%b')) # 4. Cost by Tag/Department ax4 = axes[1, 1] departments = ['Engineering', 'Data Science', 'QA', 'DevOps', 'Other'] costs = [5500, 3200, 1800, 2500, 800] budgets = [5000, 3500, 2000, 2000, 1000] x = np.arange(len(departments)) width = 0.35 bars1 = ax4.bar(x - width/2, costs, width, label='Actual', color='#3498DB', alpha=0.7) bars2 = ax4.bar(x + width/2, budgets, width, label='Budget', color='#95A5A6', alpha=0.7) # Highlight over-budget departments for i, (cost, budget) in enumerate(zip(costs, budgets)): if cost \u003e budget: ax4.plot(i, cost + 200, 'r^', markersize=10) ax4.text(i, cost + 400, f'+${cost-budget}', ha='center', color='red', fontweight='bold') ax4.set_title('Cost by Department vs Budget', fontweight='bold') ax4.set_ylabel('Cost ($)') ax4.set_xticks(x) ax4.set_xticklabels(departments, rotation=45, ha='right') ax4.legend() ax4.grid(True, alpha=0.3, axis='y') plt.tight_layout() return fig # Create cost optimization visualization cost_viz = visualize_cost_optimization() plt.show() ","references-and-learning-resources#References and Learning Resources":" Matplotlib Official Documentation Matplotlib Gallery Seaborn for Statistical Visualizations Real Python - Matplotlib Guide Effective Data Visualization (Tufte) Python Graph Gallery Plotly for Interactive Dashboards Grafana Python SDK Prometheus Python Client "},"title":"Matplotlib for Monitoring Dashboards"},"/learn-ai/docs/fundamentals/week1/numpy-arrays/":{"data":{"":"","core-concepts-with-devops-applications#Core Concepts with DevOps Applications":"1. Arrays for Metrics Collection DevOps Context: Storing and processing time-series metrics\nimport numpy as np from datetime import datetime, timedelta # CPU metrics collected every minute for the last hour cpu_metrics = np.array([ 45.2, 48.3, 52.1, 67.8, 72.3, 68.9, 71.2, 75.6, 82.3, 88.1, 92.5, 94.2, 91.8, 87.3, 83.2, 79.5, 76.8, 73.2, 69.8, 66.4, 62.1, 58.9, 55.3, 52.8, 49.2, 46.8, 44.3, 42.1, 40.8, 39.2, 38.5, 37.9, 38.2, 39.8, 42.3, 45.6, 48.9, 52.3, 56.7, 61.2, 65.8, 70.3, 74.8, 78.2, 81.5, 84.2, 86.3, 87.8, 88.9, 89.5, 90.1, 89.8, 88.3, 86.2, 83.8, 81.2, 78.5, 75.8, 73.2, 70.8 ]) # Quick statistics print(f\"Average CPU: {np.mean(cpu_metrics):.2f}%\") print(f\"Peak CPU: {np.max(cpu_metrics):.2f}%\") print(f\"Min CPU: {np.min(cpu_metrics):.2f}%\") print(f\"Std Dev: {np.std(cpu_metrics):.2f}\") # Find periods of high load (\u003e 80%) high_load_indices = np.where(cpu_metrics \u003e 80)[0] print(f\"High load periods (minutes): {high_load_indices}\") 2. Multi-dimensional Arrays for Server Farms DevOps Context: Managing metrics across multiple servers\n# Metrics for 10 servers over 24 hours (hourly samples) # Dimensions: [servers, hours, metrics_type] # metrics_type: 0=CPU, 1=Memory, 2=Disk I/O server_metrics = np.random.rand(10, 24, 3) * 100 # Server names for reference server_names = [f\"web-{i:02d}\" for i in range(10)] def analyze_server_health(metrics, server_names): \"\"\"Analyze health metrics across server farm\"\"\" # Average metrics per server avg_per_server = np.mean(metrics, axis=1) # Find servers with high average CPU (\u003e 75%) high_cpu_servers = np.where(avg_per_server[:, 0] \u003e 75)[0] # Find servers with high memory usage (\u003e 85%) high_mem_servers = np.where(avg_per_server[:, 1] \u003e 85)[0] # Calculate server health score (lower is better) health_scores = np.sum(avg_per_server, axis=1) / 3 # Rank servers by health ranked_indices = np.argsort(health_scores)[::-1] print(\"Server Health Analysis:\") print(\"-\" * 40) for idx in ranked_indices[:5]: # Top 5 problematic servers print(f\"{server_names[idx]}: Score={health_scores[idx]:.1f} \" f\"CPU={avg_per_server[idx, 0]:.1f}% \" f\"Mem={avg_per_server[idx, 1]:.1f}% \" f\"IO={avg_per_server[idx, 2]:.1f}%\") return { \"high_cpu\": [server_names[i] for i in high_cpu_servers], \"high_memory\": [server_names[i] for i in high_mem_servers], \"health_scores\": dict(zip(server_names, health_scores)) } # Analyze the server farm results = analyze_server_health(server_metrics, server_names) 3. Statistical Operations for Anomaly Detection DevOps Context: Detecting unusual patterns in system behavior\ndef detect_anomalies(metrics, window_size=10, threshold=3): \"\"\" Detect anomalies using moving average and standard deviation Used for identifying unusual spikes in metrics \"\"\" # Calculate moving average moving_avg = np.convolve(metrics, np.ones(window_size)/window_size, mode='valid') # Calculate moving standard deviation moving_std = np.array([ np.std(metrics[i:i+window_size]) for i in range(len(metrics) - window_size + 1) ]) # Detect anomalies (values beyond threshold * std from mean) anomalies = [] for i in range(len(moving_avg)): actual_value = metrics[i + window_size - 1] if abs(actual_value - moving_avg[i]) \u003e threshold * moving_std[i]: anomalies.append({ 'index': i + window_size - 1, 'value': actual_value, 'expected': moving_avg[i], 'deviation': abs(actual_value - moving_avg[i]) / moving_std[i] }) return anomalies # Example: Detect anomalies in response times response_times = np.random.normal(100, 15, 1000) # Normal: 100ms ¬± 15ms # Inject some anomalies response_times[150] = 400 # Spike response_times[500] = 350 # Another spike response_times[750] = 10 # Drop anomalies = detect_anomalies(response_times) print(f\"Found {len(anomalies)} anomalies in response times\") for anomaly in anomalies[:5]: print(f\" Time index {anomaly['index']}: {anomaly['value']:.1f}ms \" f\"(expected: {anomaly['expected']:.1f}ms, \" f\"{anomaly['deviation']:.1f} std devs)\") 4. Array Operations for Capacity Planning DevOps Context: Predicting resource needs based on historical data\ndef capacity_planning(historical_usage, growth_rate=0.1, forecast_days=30): \"\"\" Predict future capacity needs based on historical usage patterns \"\"\" # Calculate trend using linear regression days = np.arange(len(historical_usage)) coefficients = np.polyfit(days, historical_usage, 1) trend_line = np.poly1d(coefficients) # Project future usage future_days = np.arange(len(historical_usage), len(historical_usage) + forecast_days) projected_usage = trend_line(future_days) # Add growth factor projected_usage *= (1 + growth_rate) # Calculate required capacity (with 20% buffer) required_capacity = np.max(projected_usage) * 1.2 # Find when we'll exceed current capacity current_capacity = 1000 # GB days_until_capacity = np.where(projected_usage \u003e current_capacity)[0] return { 'current_avg': np.mean(historical_usage), 'projected_avg': np.mean(projected_usage), 'peak_projected': np.max(projected_usage), 'required_capacity': required_capacity, 'days_until_capacity_exceeded': days_until_capacity[0] if len(days_until_capacity) \u003e 0 else None } # Historical storage usage (GB) over 90 days storage_usage = np.linspace(600, 850, 90) + np.random.normal(0, 20, 90) planning = capacity_planning(storage_usage) print(\"Capacity Planning Report:\") print(f\" Current Average: {planning['current_avg']:.1f} GB\") print(f\" 30-day Projected Average: {planning['projected_avg']:.1f} GB\") print(f\" Required Capacity: {planning['required_capacity']:.1f} GB\") if planning['days_until_capacity_exceeded']: print(f\" WARNING: Will exceed capacity in {planning['days_until_capacity_exceeded']} days!\") 5. Performance Optimization with NumPy DevOps Context: Efficient processing of large-scale metrics\nimport time def process_metrics_python(metrics): \"\"\"Process metrics using pure Python (slow)\"\"\" result = [] for value in metrics: if value \u003e 50: result.append(value * 1.1) else: result.append(value * 0.9) return result def process_metrics_numpy(metrics): \"\"\"Process metrics using NumPy (fast)\"\"\" result = np.where(metrics \u003e 50, metrics * 1.1, metrics * 0.9) return result # Compare performance large_metrics = np.random.rand(1000000) * 100 # Python approach start = time.time() python_result = process_metrics_python(large_metrics.tolist()) python_time = time.time() - start # NumPy approach start = time.time() numpy_result = process_metrics_numpy(large_metrics) numpy_time = time.time() - start print(f\"Python processing time: {python_time:.4f} seconds\") print(f\"NumPy processing time: {numpy_time:.4f} seconds\") print(f\"Speed improvement: {python_time/numpy_time:.1f}x faster\") ","overview#Overview":"NumPy is essential for efficient numerical operations in DevOps/SRE contexts, particularly when dealing with large-scale metrics, performance data, and system monitoring. It provides the foundation for analyzing infrastructure patterns and anomalies.","practice-exercises#Practice Exercises":" Metric Aggregation: Write a function that aggregates metrics from multiple data centers and calculates weighted averages based on traffic volume.\nTrend Detection: Implement a function that detects upward or downward trends in system metrics using linear regression.\nResource Correlation: Create a script that finds correlations between different metrics (CPU, memory, network) to identify resource bottlenecks.\nPercentile Monitoring: Build a monitoring system that tracks percentile-based SLIs (Service Level Indicators) over time.","real-world-devops-examples#Real-World DevOps Examples":"Example 1: Network Traffic Analysis import numpy as np class NetworkTrafficAnalyzer: \"\"\"Analyze network traffic patterns for capacity planning\"\"\" def __init__(self, sampling_rate=60): # seconds self.sampling_rate = sampling_rate self.traffic_data = [] def add_sample(self, bytes_in, bytes_out, packets_in, packets_out): \"\"\"Add a traffic sample\"\"\" self.traffic_data.append([bytes_in, bytes_out, packets_in, packets_out]) def analyze_patterns(self): \"\"\"Analyze traffic patterns\"\"\" if len(self.traffic_data) \u003c 2: return None data = np.array(self.traffic_data) # Calculate bandwidth utilization (Mbps) bytes_total = data[:, 0] + data[:, 1] bandwidth_mbps = (bytes_total * 8) / (self.sampling_rate * 1_000_000) # Packet analysis packets_total = data[:, 2] + data[:, 3] avg_packet_size = bytes_total / np.maximum(packets_total, 1) # Detect traffic spikes (\u003e2 std dev from mean) mean_traffic = np.mean(bandwidth_mbps) std_traffic = np.std(bandwidth_mbps) spikes = np.where(bandwidth_mbps \u003e mean_traffic + 2 * std_traffic)[0] # Calculate percentiles for SLA monitoring percentiles = np.percentile(bandwidth_mbps, [50, 95, 99]) return { 'avg_bandwidth_mbps': mean_traffic, 'peak_bandwidth_mbps': np.max(bandwidth_mbps), 'p50_bandwidth': percentiles[0], 'p95_bandwidth': percentiles[1], 'p99_bandwidth': percentiles[2], 'avg_packet_size': np.mean(avg_packet_size), 'traffic_spikes': len(spikes), 'spike_indices': spikes.tolist() } def predict_peak_hours(self, hourly_data): \"\"\"Identify peak traffic hours\"\"\" hourly_avg = np.mean(hourly_data.reshape(-1, 24), axis=0) peak_hours = np.argsort(hourly_avg)[-3:] # Top 3 hours return peak_hours, hourly_avg[peak_hours] # Example usage analyzer = NetworkTrafficAnalyzer() # Simulate network traffic data for _ in range(1440): # 24 hours of minute-by-minute data bytes_in = np.random.exponential(1000000) # Exponential distribution for traffic bytes_out = np.random.exponential(500000) packets_in = int(bytes_in / 1500) # Assuming ~1500 byte packets packets_out = int(bytes_out / 1500) analyzer.add_sample(bytes_in, bytes_out, packets_in, packets_out) results = analyzer.analyze_patterns() print(\"Network Traffic Analysis:\") for key, value in results.items(): if isinstance(value, float): print(f\" {key}: {value:.2f}\") else: print(f\" {key}: {value}\") Example 2: Load Balancer Distribution Analysis def analyze_load_distribution(request_counts, server_names): \"\"\" Analyze how well load is distributed across servers \"\"\" total_requests = np.sum(request_counts) expected_per_server = total_requests / len(request_counts) # Calculate distribution metrics distribution = request_counts / total_requests * 100 std_dev = np.std(request_counts) cv = (std_dev / np.mean(request_counts)) * 100 # Coefficient of variation # Chi-square test for uniformity chi_square = np.sum((request_counts - expected_per_server) ** 2 / expected_per_server) # Identify over/under utilized servers deviation_pct = ((request_counts - expected_per_server) / expected_per_server) * 100 overloaded = np.where(deviation_pct \u003e 20)[0] underutilized = np.where(deviation_pct \u003c -20)[0] print(\"Load Balancer Analysis:\") print(f\" Total Requests: {total_requests:,.0f}\") print(f\" Expected per server: {expected_per_server:,.0f}\") print(f\" Standard Deviation: {std_dev:,.0f}\") print(f\" Coefficient of Variation: {cv:.1f}%\") print(f\" Chi-square statistic: {chi_square:.2f}\") if len(overloaded) \u003e 0: print(f\"\\n Overloaded servers (\u003e20% above expected):\") for idx in overloaded: print(f\" {server_names[idx]}: {request_counts[idx]:,.0f} \" f\"({deviation_pct[idx]:+.1f}%)\") if len(underutilized) \u003e 0: print(f\"\\n Underutilized servers (\u003e20% below expected):\") for idx in underutilized: print(f\" {server_names[idx]}: {request_counts[idx]:,.0f} \" f\"({deviation_pct[idx]:+.1f}%)\") return { 'distribution': distribution, 'cv': cv, 'chi_square': chi_square, 'balanced': cv \u003c 10 # Consider balanced if CV \u003c 10% } # Example: Analyze load distribution across 8 servers server_names = [f\"app-{i:02d}\" for i in range(8)] request_counts = np.array([98500, 102300, 99800, 121000, 95600, 97200, 103400, 98200]) analysis = analyze_load_distribution(request_counts, server_names) Example 3: SLA Compliance Monitoring def calculate_sla_metrics(response_times, sla_target=200): \"\"\" Calculate SLA compliance metrics for response times \"\"\" # Remove outliers using IQR method q1, q3 = np.percentile(response_times, [25, 75]) iqr = q3 - q1 lower_bound = q1 - 1.5 * iqr upper_bound = q3 + 1.5 * iqr filtered_times = response_times[(response_times \u003e= lower_bound) \u0026 (response_times \u003c= upper_bound)] # Calculate percentiles percentiles = np.percentile(filtered_times, [50, 90, 95, 99, 99.9]) # SLA compliance compliance_rate = (np.sum(response_times \u003c= sla_target) / len(response_times)) * 100 # Apdex score (Application Performance Index) satisfied = np.sum(response_times \u003c= sla_target) tolerating = np.sum((response_times \u003e sla_target) \u0026 (response_times \u003c= sla_target * 4)) apdex = (satisfied + tolerating * 0.5) / len(response_times) print(\"SLA Compliance Report:\") print(f\" Target SLA: {sla_target}ms\") print(f\" Compliance Rate: {compliance_rate:.2f}%\") print(f\" Apdex Score: {apdex:.3f}\") print(f\" Median (P50): {percentiles[0]:.1f}ms\") print(f\" P90: {percentiles[1]:.1f}ms\") print(f\" P95: {percentiles[2]:.1f}ms\") print(f\" P99: {percentiles[3]:.1f}ms\") print(f\" P99.9: {percentiles[4]:.1f}ms\") return { 'compliance_rate': compliance_rate, 'apdex': apdex, 'percentiles': dict(zip(['p50', 'p90', 'p95', 'p99', 'p99.9'], percentiles)) } # Generate sample response times (mix of normal and some slow requests) normal_responses = np.random.normal(150, 30, 9500) slow_responses = np.random.normal(400, 100, 500) response_times = np.concatenate([normal_responses, slow_responses]) np.random.shuffle(response_times) sla_metrics = calculate_sla_metrics(response_times) ","references-and-learning-resources#References and Learning Resources":" NumPy Official Documentation NumPy for Data Science Scientific Computing with NumPy NumPy Illustrated: The Visual Guide Performance Python: NumPy Time Series Analysis with NumPy NumPy Financial Functions NumPy Random Sampling for Simulations "},"title":"NumPy for Infrastructure Metrics"},"/learn-ai/docs/fundamentals/week1/pandas-dataframes/":{"data":{"":"","core-concepts-with-devops-applications#Core Concepts with DevOps Applications":"1. DataFrames for Log Analysis DevOps Context: Parsing and analyzing application logs\nimport pandas as pd import numpy as np from datetime import datetime, timedelta import re # Sample log data log_data = \"\"\" 2024-01-15 10:23:45 INFO [api-gateway] Request from 192.168.1.100 - GET /api/users - 200 - 145ms 2024-01-15 10:23:46 ERROR [auth-service] Authentication failed for user john.doe - 401 - 23ms 2024-01-15 10:23:47 INFO [api-gateway] Request from 192.168.1.101 - POST /api/orders - 201 - 523ms 2024-01-15 10:23:48 WARN [database] Query timeout on orders table - 1245ms 2024-01-15 10:23:49 INFO [api-gateway] Request from 192.168.1.100 - GET /api/products - 200 - 89ms 2024-01-15 10:23:50 ERROR [payment-service] Payment processing failed - Transaction ID: TX123456 - 500 - 2341ms 2024-01-15 10:23:51 INFO [api-gateway] Request from 192.168.1.102 - GET /api/users/123 - 404 - 12ms 2024-01-15 10:23:52 WARN [cache-service] Cache miss rate high: 45% - Performance degradation expected 2024-01-15 10:23:53 INFO [api-gateway] Request from 192.168.1.100 - DELETE /api/sessions - 204 - 34ms 2024-01-15 10:23:54 ERROR [api-gateway] Request from 192.168.1.103 - GET /api/reports - 503 - Service Unavailable \"\"\" def parse_logs_to_dataframe(log_text): \"\"\"Parse log text into a structured DataFrame\"\"\" lines = log_text.strip().split('\\n') parsed_logs = [] for line in lines: # Parse log line using regex pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (\\w+) \\[([^\\]]+)\\] (.+)' match = re.match(pattern, line) if match: timestamp_str, level, service, message = match.groups() # Extract response time if present time_match = re.search(r'(\\d+)ms', message) response_time = int(time_match.group(1)) if time_match else None # Extract status code if present status_match = re.search(r'\\b(\\d{3})\\b', message) status_code = int(status_match.group(1)) if status_match else None parsed_logs.append({ 'timestamp': pd.to_datetime(timestamp_str), 'level': level, 'service': service, 'message': message, 'response_time_ms': response_time, 'status_code': status_code }) return pd.DataFrame(parsed_logs) # Parse logs into DataFrame df_logs = parse_logs_to_dataframe(log_data) # Analyze log data print(\"Log Analysis Summary:\") print(\"-\" * 50) print(f\"Total log entries: {len(df_logs)}\") print(f\"\\nLog levels distribution:\") print(df_logs['level'].value_counts()) print(f\"\\nServices with errors:\") error_services = df_logs[df_logs['level'] == 'ERROR']['service'].value_counts() print(error_services) print(f\"\\nAverage response time by service:\") response_times = df_logs.groupby('service')['response_time_ms'].mean().dropna() print(response_times.round(2)) 2. Time Series Analysis for Metrics DevOps Context: Analyzing infrastructure metrics over time\n# Generate sample infrastructure metrics def generate_infrastructure_metrics(): \"\"\"Generate realistic infrastructure metrics data\"\"\" # Create time index date_range = pd.date_range( start='2024-01-01 00:00:00', end='2024-01-07 23:59:59', freq='1H' ) # Generate metrics with daily and weekly patterns n_points = len(date_range) # CPU usage with daily pattern (higher during business hours) base_cpu = 30 daily_pattern = np.sin(np.arange(n_points) * 2 * np.pi / 24) * 20 weekly_pattern = np.where( pd.Series(date_range).dt.dayofweek.isin([5, 6]), # Weekend -10, 0 ) cpu_usage = base_cpu + daily_pattern + weekly_pattern + np.random.normal(0, 5, n_points) cpu_usage = np.clip(cpu_usage, 5, 95) # Memory usage (gradual increase with occasional drops) memory_usage = 40 + np.cumsum(np.random.normal(0.1, 2, n_points)) / 50 memory_usage = np.clip(memory_usage, 20, 90) # Request count (business hours pattern) hour_of_day = pd.Series(date_range).dt.hour is_business_hours = (hour_of_day \u003e= 8) \u0026 (hour_of_day \u003c= 18) is_weekday = ~pd.Series(date_range).dt.dayofweek.isin([5, 6]) request_count = np.where( is_business_hours \u0026 is_weekday, np.random.poisson(1000, n_points), np.random.poisson(200, n_points) ) # Error rate (increases with load) error_rate = (request_count / 1000) * np.random.uniform(0.5, 2, n_points) df = pd.DataFrame({ 'timestamp': date_range, 'cpu_usage': cpu_usage, 'memory_usage': memory_usage, 'request_count': request_count, 'error_rate': error_rate, 'response_time_p99': 100 + (cpu_usage * 2) + np.random.normal(0, 20, n_points) }) return df # Create metrics DataFrame df_metrics = generate_infrastructure_metrics() # Set timestamp as index for time series operations df_metrics.set_index('timestamp', inplace=True) # Resample to different time windows hourly_avg = df_metrics.resample('1H').mean() daily_avg = df_metrics.resample('1D').mean() daily_max = df_metrics.resample('1D').max() print(\"Infrastructure Metrics Analysis:\") print(\"-\" * 50) print(\"\\nDaily Average Metrics:\") print(daily_avg[['cpu_usage', 'memory_usage', 'error_rate']].round(2)) # Identify peak usage times peak_hours = df_metrics.groupby(df_metrics.index.hour)['request_count'].mean().sort_values(ascending=False) print(f\"\\nTop 5 Peak Hours (by average request count):\") print(peak_hours.head().round(0)) # Correlation analysis print(\"\\nMetric Correlations:\") correlations = df_metrics[['cpu_usage', 'memory_usage', 'request_count', 'error_rate', 'response_time_p99']].corr() print(correlations.round(3)) 3. Incident Analysis and Reporting DevOps Context: Tracking and analyzing production incidents\n# Create incident tracking DataFrame incidents_data = { 'incident_id': ['INC001', 'INC002', 'INC003', 'INC004', 'INC005', 'INC006', 'INC007', 'INC008'], 'timestamp': pd.to_datetime([ '2024-01-01 03:45:00', '2024-01-02 14:30:00', '2024-01-03 09:15:00', '2024-01-04 22:00:00', '2024-01-05 11:20:00', '2024-01-06 16:45:00', '2024-01-07 08:30:00', '2024-01-08 20:15:00' ]), 'service': ['auth-service', 'database', 'api-gateway', 'payment-service', 'cache-service', 'database', 'auth-service', 'api-gateway'], 'severity': ['P1', 'P2', 'P3', 'P1', 'P2', 'P1', 'P3', 'P2'], 'duration_minutes': [45, 120, 15, 180, 60, 240, 30, 90], 'root_cause': ['Memory leak', 'Slow queries', 'Network timeout', 'Third-party API down', 'Redis connection pool', 'Disk space', 'Certificate expired', 'DDoS attack'], 'affected_users': [5000, 2000, 500, 10000, 3000, 15000, 1000, 8000], 'resolved_by': ['John', 'Sarah', 'Mike', 'John', 'Sarah', 'Mike', 'John', 'Sarah'] } df_incidents = pd.DataFrame(incidents_data) # Add calculated fields df_incidents['mttr_hours'] = df_incidents['duration_minutes'] / 60 df_incidents['impact_score'] = ( df_incidents['affected_users'] * df_incidents['severity'].map({'P1': 3, 'P2': 2, 'P3': 1}) * df_incidents['duration_minutes'] / 60 ) # Incident analysis print(\"Incident Analysis Report:\") print(\"-\" * 50) # MTTR by severity mttr_by_severity = df_incidents.groupby('severity')['mttr_hours'].agg(['mean', 'median', 'max']) print(\"\\nMTTR by Severity (hours):\") print(mttr_by_severity.round(2)) # Most problematic services service_incidents = df_incidents.groupby('service').agg({ 'incident_id': 'count', 'duration_minutes': 'sum', 'affected_users': 'sum', 'impact_score': 'sum' }).rename(columns={'incident_id': 'incident_count', 'duration_minutes': 'total_downtime_min'}) service_incidents = service_incidents.sort_values('impact_score', ascending=False) print(\"\\nService Reliability Report:\") print(service_incidents) # Root cause analysis root_cause_freq = df_incidents['root_cause'].value_counts() print(\"\\nTop Root Causes:\") print(root_cause_freq) # On-call performance oncall_stats = df_incidents.groupby('resolved_by').agg({ 'incident_id': 'count', 'mttr_hours': 'mean', 'severity': lambda x: (x == 'P1').sum() }).rename(columns={'incident_id': 'incidents_resolved', 'severity': 'p1_incidents'}) print(\"\\nOn-Call Engineer Performance:\") print(oncall_stats.round(2)) 4. Deployment Tracking and Analysis DevOps Context: Monitoring deployment success rates and rollback patterns\n# Create deployment tracking data deployments = { 'deployment_id': [f'DEP{i:04d}' for i in range(1, 51)], 'timestamp': pd.date_range(start='2024-01-01', periods=50, freq='4H'), 'service': np.random.choice(['api', 'auth', 'database', 'frontend', 'backend'], 50), 'environment': np.random.choice(['dev', 'staging', 'production'], 50, p=[0.4, 0.3, 0.3]), 'version': [f'v1.{i//10}.{i%10}' for i in range(50)], 'deployment_method': np.random.choice(['blue-green', 'canary', 'rolling'], 50), 'duration_seconds': np.random.normal(300, 100, 50).clip(60, 600), 'status': np.random.choice(['success', 'failed', 'rolled_back'], 50, p=[0.8, 0.1, 0.1]), 'deployed_by': np.random.choice(['CI/CD', 'Manual'], 50, p=[0.7, 0.3]) } df_deployments = pd.DataFrame(deployments) # Add calculated fields df_deployments['date'] = df_deployments['timestamp'].dt.date df_deployments['hour'] = df_deployments['timestamp'].dt.hour df_deployments['weekday'] = df_deployments['timestamp'].dt.day_name() df_deployments['is_business_hours'] = df_deployments['hour'].between(9, 17) # Deployment success analysis print(\"Deployment Analysis Report:\") print(\"-\" * 50) # Success rate by environment success_by_env = pd.crosstab( df_deployments['environment'], df_deployments['status'], normalize='index' ) * 100 print(\"\\nSuccess Rate by Environment (%):\") print(success_by_env.round(1)) # Deployment frequency and patterns deploy_by_day = df_deployments.groupby('weekday')['deployment_id'].count() deploy_by_day = deploy_by_day.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']) print(\"\\nDeployments by Day of Week:\") print(deploy_by_day) # Best practices compliance df_deployments['follows_best_practice'] = ( (df_deployments['environment'] != 'production') | (df_deployments['is_business_hours'] \u0026 (df_deployments['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday']))) ) compliance_rate = df_deployments['follows_best_practice'].mean() * 100 print(f\"\\nBest Practice Compliance Rate: {compliance_rate:.1f}%\") # Service deployment reliability service_reliability = df_deployments.groupby('service').agg({ 'status': lambda x: (x == 'success').mean() * 100, 'duration_seconds': 'mean', 'deployment_id': 'count' }).rename(columns={ 'status': 'success_rate', 'duration_seconds': 'avg_duration', 'deployment_id': 'total_deployments' }) print(\"\\nService Deployment Reliability:\") print(service_reliability.round(1)) 5. Cost Analysis and Optimization DevOps Context: Analyzing cloud infrastructure costs\n# Generate cloud cost data def generate_cloud_costs(): dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D') services = [] for date in dates: # Compute costs (higher on weekdays) is_weekday = date.weekday() \u003c 5 compute_base = 500 if is_weekday else 300 services.append({ 'date': date, 'service': 'EC2', 'cost': compute_base + np.random.normal(0, 50), 'usage_hours': (24 * 20) if is_weekday else (24 * 10), 'region': 'us-east-1' }) # Storage costs (gradually increasing) day_num = (date - dates[0]).days services.append({ 'date': date, 'service': 'S3', 'cost': 100 + day_num * 2 + np.random.normal(0, 10), 'usage_gb': 5000 + day_num * 100, 'region': 'us-east-1' }) # Database costs services.append({ 'date': date, 'service': 'RDS', 'cost': 200 + np.random.normal(0, 20), 'usage_hours': 24, 'region': 'us-east-1' }) # Network costs services.append({ 'date': date, 'service': 'CloudFront', 'cost': 50 + np.random.exponential(30), 'usage_gb': np.random.exponential(1000), 'region': 'global' }) return pd.DataFrame(services) df_costs = generate_cloud_costs() # Cost analysis print(\"Cloud Cost Analysis:\") print(\"-\" * 50) # Total costs by service total_by_service = df_costs.groupby('service')['cost'].sum().sort_values(ascending=False) print(\"\\nTotal Costs by Service (January):\") print(total_by_service.round(2)) # Daily cost trends daily_costs = df_costs.groupby('date')['cost'].sum() print(f\"\\nDaily Cost Statistics:\") print(f\" Average: ${daily_costs.mean():.2f}\") print(f\" Minimum: ${daily_costs.min():.2f}\") print(f\" Maximum: ${daily_costs.max():.2f}\") print(f\" Total: ${daily_costs.sum():.2f}\") # Cost efficiency metrics df_costs['cost_per_unit'] = df_costs.apply( lambda row: row['cost'] / row.get('usage_hours', row.get('usage_gb', 1)), axis=1 ) efficiency = df_costs.groupby('service')['cost_per_unit'].mean() print(\"\\nCost Efficiency (per unit):\") print(efficiency.round(4)) # Identify cost anomalies service_daily_avg = df_costs.groupby(['service', 'date'])['cost'].sum().reset_index() for service in df_costs['service'].unique(): service_data = service_daily_avg[service_daily_avg['service'] == service]['cost'] mean_cost = service_data.mean() std_cost = service_data.std() anomalies = service_data[service_data \u003e mean_cost + 2 * std_cost] if len(anomalies) \u003e 0: print(f\"\\nCost anomalies detected for {service}:\") print(f\" Normal range: ${mean_cost - 2*std_cost:.2f} - ${mean_cost + 2*std_cost:.2f}\") print(f\" Anomalies found: {len(anomalies)} days\") ","overview#Overview":"Pandas is the go-to library for analyzing structured data in DevOps/SRE contexts. It excels at processing logs, analyzing incidents, tracking deployments, and generating reports from various infrastructure data sources.","practice-exercises#Practice Exercises":" Log Aggregation Pipeline: Build a pipeline that ingests logs from multiple sources, normalizes them, and creates a unified dashboard dataset.\nCapacity Forecasting: Use historical data to predict when resources will need scaling based on growth trends.\nIncident Correlation: Analyze incident data to find patterns and correlations between different types of failures.\nCost Optimization Report: Create a comprehensive cost analysis report that identifies optimization opportunities.","real-world-devops-examples#Real-World DevOps Examples":"Example 1: API Performance Dashboard Data Preparation class APIPerformanceAnalyzer: \"\"\"Analyze API performance metrics from logs\"\"\" def __init__(self): self.df = None def load_api_logs(self, log_file=None): \"\"\"Load and parse API access logs\"\"\" # Simulate API log data n_records = 10000 endpoints = ['/api/users', '/api/orders', '/api/products', '/api/auth/login', '/api/health'] methods = ['GET', 'POST', 'PUT', 'DELETE'] data = { 'timestamp': pd.date_range(start='2024-01-01', periods=n_records, freq='1min'), 'endpoint': np.random.choice(endpoints, n_records, p=[0.3, 0.25, 0.25, 0.15, 0.05]), 'method': np.random.choice(methods, n_records, p=[0.5, 0.3, 0.15, 0.05]), 'status_code': np.random.choice([200, 201, 400, 401, 404, 500, 503], n_records, p=[0.7, 0.1, 0.05, 0.03, 0.05, 0.05, 0.02]), 'response_time_ms': np.random.lognormal(4, 1, n_records), 'user_id': np.random.choice(range(1, 1001), n_records), 'ip_address': [f\"192.168.1.{np.random.randint(1, 255)}\" for _ in range(n_records)] } self.df = pd.DataFrame(data) self.df['success'] = self.df['status_code'] \u003c 400 self.df['hour'] = self.df['timestamp'].dt.hour def calculate_sli_metrics(self, time_window='1H'): \"\"\"Calculate Service Level Indicators\"\"\" # Resample data by time window resampled = self.df.set_index('timestamp').resample(time_window) sli_metrics = pd.DataFrame({ 'availability': resampled['success'].mean() * 100, 'p50_latency': resampled['response_time_ms'].quantile(0.5), 'p95_latency': resampled['response_time_ms'].quantile(0.95), 'p99_latency': resampled['response_time_ms'].quantile(0.99), 'error_rate': (1 - resampled['success'].mean()) * 100, 'request_rate': resampled.size() / (pd.Timedelta(time_window).seconds / 60) # requests per minute }) return sli_metrics.round(2) def endpoint_analysis(self): \"\"\"Analyze performance by endpoint\"\"\" endpoint_stats = self.df.groupby('endpoint').agg({ 'response_time_ms': ['mean', 'median', lambda x: x.quantile(0.95)], 'success': lambda x: x.mean() * 100, 'status_code': 'count' }).round(2) endpoint_stats.columns = ['avg_response', 'median_response', 'p95_response', 'success_rate', 'request_count'] # Add error breakdown error_breakdown = self.df[~self.df['success']].groupby(['endpoint', 'status_code']).size().unstack(fill_value=0) return endpoint_stats, error_breakdown def identify_problematic_users(self, threshold_error_rate=20): \"\"\"Identify users with high error rates\"\"\" user_stats = self.df.groupby('user_id').agg({ 'success': lambda x: (1 - x.mean()) * 100, # error rate 'response_time_ms': 'mean', 'status_code': 'count' }).rename(columns={'success': 'error_rate', 'status_code': 'request_count'}) problematic_users = user_stats[user_stats['error_rate'] \u003e threshold_error_rate] return problematic_users.sort_values('error_rate', ascending=False) # Use the analyzer analyzer = APIPerformanceAnalyzer() analyzer.load_api_logs() # Get SLI metrics sli_metrics = analyzer.calculate_sli_metrics('1H') print(\"Hourly SLI Metrics (last 5 hours):\") print(sli_metrics.tail()) # Analyze endpoints endpoint_stats, error_breakdown = analyzer.endpoint_analysis() print(\"\\nEndpoint Performance Summary:\") print(endpoint_stats) # Find problematic users problematic_users = analyzer.identify_problematic_users() if len(problematic_users) \u003e 0: print(f\"\\nUsers with \u003e20% error rate: {len(problematic_users)}\") print(problematic_users.head()) Example 2: Kubernetes Pod Resource Analysis def analyze_k8s_metrics(): \"\"\"Analyze Kubernetes pod metrics\"\"\" # Simulate K8s pod metrics pods = [] namespaces = ['default', 'production', 'staging', 'monitoring'] for i in range(100): namespace = np.random.choice(namespaces) pods.append({ 'pod_name': f\"{namespace}-app-{i:03d}\", 'namespace': namespace, 'node': f\"node-{np.random.randint(1, 11):02d}\", 'cpu_request_m': np.random.choice([100, 250, 500, 1000]), 'cpu_usage_m': np.random.exponential(200), 'memory_request_mi': np.random.choice([128, 256, 512, 1024]), 'memory_usage_mi': np.random.normal(300, 100), 'restart_count': np.random.poisson(0.5), 'age_days': np.random.exponential(30), 'status': np.random.choice(['Running', 'Pending', 'CrashLoopBackOff', 'Error'], p=[0.9, 0.05, 0.03, 0.02]) }) df_pods = pd.DataFrame(pods) # Calculate resource efficiency df_pods['cpu_efficiency'] = (df_pods['cpu_usage_m'] / df_pods['cpu_request_m']) * 100 df_pods['memory_efficiency'] = (df_pods['memory_usage_mi'] / df_pods['memory_request_mi']) * 100 # Identify resource issues df_pods['over_cpu'] = df_pods['cpu_efficiency'] \u003e 90 df_pods['under_cpu'] = df_pods['cpu_efficiency'] \u003c 10 df_pods['over_memory'] = df_pods['memory_efficiency'] \u003e 90 df_pods['under_memory'] = df_pods['memory_efficiency'] \u003c 10 print(\"Kubernetes Resource Analysis:\") print(\"-\" * 50) # Namespace summary namespace_summary = df_pods.groupby('namespace').agg({ 'pod_name': 'count', 'cpu_usage_m': 'sum', 'memory_usage_mi': 'sum', 'restart_count': 'sum', 'status': lambda x: (x != 'Running').sum() }).rename(columns={ 'pod_name': 'pod_count', 'cpu_usage_m': 'total_cpu_m', 'memory_usage_mi': 'total_memory_mi', 'restart_count': 'total_restarts', 'status': 'unhealthy_pods' }) print(\"\\nNamespace Resource Usage:\") print(namespace_summary) # Node distribution node_load = df_pods.groupby('node').agg({ 'pod_name': 'count', 'cpu_usage_m': 'sum', 'memory_usage_mi': 'sum' }).rename(columns={'pod_name': 'pod_count'}) print(\"\\nNode Load Distribution:\") print(node_load.sort_values('cpu_usage_m', ascending=False).head()) # Resource optimization opportunities print(\"\\nResource Optimization Opportunities:\") print(f\" Pods with \u003c10% CPU usage: {df_pods['under_cpu'].sum()}\") print(f\" Pods with \u003e90% CPU usage: {df_pods['over_cpu'].sum()}\") print(f\" Pods with \u003c10% Memory usage: {df_pods['under_memory'].sum()}\") print(f\" Pods with \u003e90% Memory usage: {df_pods['over_memory'].sum()}\") # Problem pods problem_pods = df_pods[ (df_pods['status'] != 'Running') | (df_pods['restart_count'] \u003e 3) | (df_pods['over_cpu']) | (df_pods['over_memory']) ][['pod_name', 'namespace', 'status', 'restart_count', 'cpu_efficiency', 'memory_efficiency']] if len(problem_pods) \u003e 0: print(f\"\\nProblem Pods ({len(problem_pods)} found):\") print(problem_pods.head(10)) return df_pods # Run the analysis k8s_metrics = analyze_k8s_metrics() ","references-and-learning-resources#References and Learning Resources":" Pandas Official Documentation Pandas for Data Analysis (Wes McKinney) Real Python - Pandas DataFrames Pandas Cookbook Time Series Analysis with Pandas Log Analysis with Python and Pandas DataCamp - Data Manipulation with Pandas Effective Pandas (Matt Harrison) "},"title":"Pandas for Log Analysis"},"/learn-ai/docs/fundamentals/week1/python-basics/":{"data":{"":"","core-concepts#Core Concepts":"1. Variables and Data Types DevOps Context: Configuration management and environment variables\n# Infrastructure configuration variables server_name = \"web-prod-01\" cpu_cores = 8 memory_gb = 16.5 is_production = True tags = [\"web\", \"production\", \"nginx\"] # Environment configuration config = { \"database_host\": \"db.example.com\", \"port\": 5432, \"max_connections\": 100, \"ssl_enabled\": True } 2. Control Flow DevOps Context: Deployment logic and health checks\n# Deployment decision logic def should_deploy(environment, tests_passed, approval_given): if environment == \"production\": if tests_passed and approval_given: return True else: print(\"Production deployment blocked: Tests or approval missing\") return False elif environment in [\"staging\", \"dev\"]: return tests_passed else: return False # Health check implementation def check_service_health(service_url, timeout=5): import requests try: response = requests.get(service_url, timeout=timeout) if response.status_code == 200: return \"healthy\" elif response.status_code \u003e= 500: return \"critical\" else: return \"degraded\" except requests.exceptions.Timeout: return \"timeout\" except Exception as e: return f\"error: {str(e)}\" 3. Functions and Modules DevOps Context: Reusable automation scripts\n# system_monitor.py - Reusable monitoring module import psutil import datetime def get_system_metrics(): \"\"\"Collect system metrics for monitoring\"\"\" return { \"timestamp\": datetime.datetime.now().isoformat(), \"cpu_percent\": psutil.cpu_percent(interval=1), \"memory_percent\": psutil.virtual_memory().percent, \"disk_usage\": psutil.disk_usage('/').percent, \"network_io\": psutil.net_io_counters()._asdict() } def alert_if_critical(metrics, thresholds): \"\"\"Send alerts if metrics exceed thresholds\"\"\" alerts = [] if metrics[\"cpu_percent\"] \u003e thresholds.get(\"cpu\", 80): alerts.append(f\"High CPU: {metrics['cpu_percent']}%\") if metrics[\"memory_percent\"] \u003e thresholds.get(\"memory\", 90): alerts.append(f\"High Memory: {metrics['memory_percent']}%\") if metrics[\"disk_usage\"] \u003e thresholds.get(\"disk\", 85): alerts.append(f\"High Disk Usage: {metrics['disk_usage']}%\") return alerts 4. Exception Handling DevOps Context: Robust error handling in automation\nimport subprocess import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def deploy_application(app_name, version): \"\"\"Deploy application with proper error handling\"\"\" try: # Pre-deployment checks logger.info(f\"Starting deployment of {app_name} v{version}\") # Pull Docker image subprocess.run( [\"docker\", \"pull\", f\"{app_name}:{version}\"], check=True, capture_output=True ) # Stop existing container try: subprocess.run( [\"docker\", \"stop\", app_name], check=True, capture_output=True, timeout=30 ) except subprocess.CalledProcessError: logger.warning(f\"No existing container for {app_name}\") # Start new container subprocess.run( [\"docker\", \"run\", \"-d\", \"--name\", app_name, f\"{app_name}:{version}\"], check=True, capture_output=True ) logger.info(f\"Successfully deployed {app_name} v{version}\") return True except subprocess.TimeoutExpired: logger.error(f\"Deployment timeout for {app_name}\") return False except subprocess.CalledProcessError as e: logger.error(f\"Deployment failed: {e.stderr.decode()}\") return False except Exception as e: logger.error(f\"Unexpected error during deployment: {str(e)}\") return False 5. File Operations DevOps Context: Configuration file management\nimport json import yaml import os class ConfigManager: \"\"\"Manage application configurations\"\"\" def __init__(self, config_dir=\"/etc/myapp\"): self.config_dir = config_dir def load_json_config(self, filename): \"\"\"Load JSON configuration file\"\"\" filepath = os.path.join(self.config_dir, filename) try: with open(filepath, 'r') as f: return json.load(f) except FileNotFoundError: logger.error(f\"Config file not found: {filepath}\") return {} except json.JSONDecodeError as e: logger.error(f\"Invalid JSON in {filepath}: {e}\") return {} def load_yaml_config(self, filename): \"\"\"Load YAML configuration file\"\"\" filepath = os.path.join(self.config_dir, filename) try: with open(filepath, 'r') as f: return yaml.safe_load(f) except FileNotFoundError: logger.error(f\"Config file not found: {filepath}\") return {} except yaml.YAMLError as e: logger.error(f\"Invalid YAML in {filepath}: {e}\") return {} def update_config(self, filename, updates): \"\"\"Update configuration file\"\"\" filepath = os.path.join(self.config_dir, filename) # Backup existing config backup_path = f\"{filepath}.backup\" if os.path.exists(filepath): with open(filepath, 'r') as src, open(backup_path, 'w') as dst: dst.write(src.read()) # Load existing config if filename.endswith('.json'): config = self.load_json_config(filename) config.update(updates) with open(filepath, 'w') as f: json.dump(config, f, indent=2) elif filename.endswith('.yaml') or filename.endswith('.yml'): config = self.load_yaml_config(filename) config.update(updates) with open(filepath, 'w') as f: yaml.dump(config, f, default_flow_style=False) ","overview#Overview":"Python is the backbone of modern DevOps and SRE practices. This guide covers essential Python concepts with real-world infrastructure automation examples.","practice-exercises#Practice Exercises":" Service Restart Automation: Write a Python script that monitors a service and automatically restarts it if it becomes unresponsive.\nConfiguration Validator: Create a script that validates YAML/JSON configuration files against a schema before deployment.\nBackup Script: Implement a backup script that archives specified directories and uploads them to S3/cloud storage.\nResource Monitor: Build a script that monitors system resources and sends alerts when thresholds are exceeded.","real-world-devops-examples#Real-World DevOps Examples":"Example 1: Automated Server Health Check Script #!/usr/bin/env python3 \"\"\" Server health check script that runs periodically via cron \"\"\" import requests import smtplib from email.mime.text import MIMEText from datetime import datetime import sys SERVERS = [ {\"name\": \"web-01\", \"url\": \"http://web-01.internal:80/health\"}, {\"name\": \"api-01\", \"url\": \"http://api-01.internal:8080/health\"}, {\"name\": \"db-01\", \"url\": \"http://db-01.internal:5432/health\"}, ] ALERT_EMAIL = \"oncall@example.com\" SMTP_SERVER = \"smtp.example.com\" def check_servers(): \"\"\"Check all servers and return status\"\"\" results = [] for server in SERVERS: try: response = requests.get(server[\"url\"], timeout=5) status = \"UP\" if response.status_code == 200 else \"DOWN\" results.append({ \"name\": server[\"name\"], \"status\": status, \"code\": response.status_code, \"response_time\": response.elapsed.total_seconds() }) except requests.exceptions.RequestException as e: results.append({ \"name\": server[\"name\"], \"status\": \"DOWN\", \"error\": str(e) }) return results def send_alert(failed_servers): \"\"\"Send email alert for failed servers\"\"\" message = f\"\"\" ALERT: Server Health Check Failed Time: {datetime.now()} Failed Servers: \"\"\" for server in failed_servers: message += f\"\\n- {server['name']}: {server.get('error', 'HTTP ' + str(server.get('code')))}\" msg = MIMEText(message) msg['Subject'] = 'Server Health Check Alert' msg['From'] = 'monitoring@example.com' msg['To'] = ALERT_EMAIL with smtplib.SMTP(SMTP_SERVER) as smtp: smtp.send_message(msg) def main(): results = check_servers() failed = [r for r in results if r[\"status\"] != \"UP\"] if failed: print(f\"CRITICAL: {len(failed)} servers are down\") send_alert(failed) sys.exit(1) else: print(f\"OK: All {len(results)} servers are healthy\") sys.exit(0) if __name__ == \"__main__\": main() Example 2: Log Rotation Script #!/usr/bin/env python3 \"\"\" Log rotation script for application logs \"\"\" import os import gzip import shutil from datetime import datetime, timedelta import glob LOG_DIR = \"/var/log/myapp\" RETENTION_DAYS = 30 MAX_SIZE_MB = 100 def rotate_log(logfile): \"\"\"Rotate a single log file\"\"\" # Generate timestamp for rotated file timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") rotated_name = f\"{logfile}.{timestamp}\" # Rename current log shutil.move(logfile, rotated_name) # Compress rotated log with open(rotated_name, 'rb') as f_in: with gzip.open(f\"{rotated_name}.gz\", 'wb') as f_out: shutil.copyfileobj(f_in, f_out) # Remove uncompressed rotated file os.remove(rotated_name) # Create new empty log file open(logfile, 'a').close() print(f\"Rotated: {logfile} -\u003e {rotated_name}.gz\") def cleanup_old_logs(): \"\"\"Remove logs older than retention period\"\"\" cutoff_date = datetime.now() - timedelta(days=RETENTION_DAYS) for compressed_log in glob.glob(f\"{LOG_DIR}/*.gz\"): file_time = datetime.fromtimestamp(os.path.getmtime(compressed_log)) if file_time \u003c cutoff_date: os.remove(compressed_log) print(f\"Deleted old log: {compressed_log}\") def main(): # Check each log file for logfile in glob.glob(f\"{LOG_DIR}/*.log\"): file_size_mb = os.path.getsize(logfile) / (1024 * 1024) if file_size_mb \u003e MAX_SIZE_MB: rotate_log(logfile) # Clean up old logs cleanup_old_logs() if __name__ == \"__main__\": main() ","references-and-learning-resources#References and Learning Resources":" Python Official Documentation Real Python - Python for DevOps Automate the Boring Stuff with Python Python for DevOps (O‚ÄôReilly Book) Google‚Äôs Python Style Guide The Hitchhiker‚Äôs Guide to Python Python Requests Library Documentation psutil Documentation (System Monitoring) "},"title":"Python Basics for DevOps/SRE"},"/learn-ai/docs/infrastructure/":{"data":{"assessment#Assessment":"","hands-on-projects#Hands-on Projects":"","infrastructure--deployment-module#Infrastructure \u0026amp; Deployment Module":"","learning-objectives#Learning Objectives":"","next-steps#Next Steps":"Proceed to Module 4: Data Engineering to master data pipeline design and feature engineering for ML systems.","performance-optimization#Performance Optimization":"","tools--resources#Tools \u0026amp; Resources":"Infrastructure \u0026 Deployment ModuleLeverage your infrastructure expertise to build scalable, reliable ML systems.\nLearning Objectives Design ML-specific infrastructure patterns Implement containerized ML deployments Orchestrate ML workloads on Kubernetes Optimize model serving for production Week 9: Containerization for ML Docker for ML Applications Multi-stage Build Pattern # Build stage FROM python:3.9 AS builder WORKDIR /app COPY requirements.txt . RUN pip install --user -r requirements.txt # Runtime stage FROM python:3.9-slim WORKDIR /app COPY --from=builder /root/.local /root/.local COPY . . ENV PATH=/root/.local/bin:$PATH CMD [\"python\", \"serve.py\"] GPU Support FROM nvidia/cuda:11.8.0-base-ubuntu22.04 # Install Python and ML frameworks RUN apt-get update \u0026\u0026 apt-get install -y python3-pip RUN pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118 Container Optimization Layer caching strategies Size reduction techniques Security scanning Registry management Week 10: Kubernetes for ML ML Workload Patterns Training Jobs apiVersion: batch/v1 kind: Job metadata: name: model-training spec: template: spec: containers: - name: trainer image: ml-training:latest resources: requests: memory: \"8Gi\" cpu: \"4\" nvidia.com/gpu: \"1\" restartPolicy: OnFailure Model Serving apiVersion: apps/v1 kind: Deployment metadata: name: model-server spec: replicas: 3 template: spec: containers: - name: model image: model-serve:latest ports: - containerPort: 8080 livenessProbe: httpGet: path: /health port: 8080 readinessProbe: httpGet: path: /ready port: 8080 Kubeflow Components Pipelines for orchestration KFServing for model serving Katib for hyperparameter tuning Notebooks for development Week 11: Model Serving Architectures Serving Frameworks Comparison Framework Use Case Pros Cons TensorFlow Serving TF models High performance TF-specific TorchServe PyTorch models Easy deployment PyTorch only Triton Multi-framework GPU optimization Complex setup Seldon Core Any framework K8s native Overhead Implementation Examples FastAPI Model Server from fastapi import FastAPI import torch import numpy as np app = FastAPI() model = torch.load(\"model.pt\") @app.post(\"/predict\") async def predict(data: dict): input_tensor = torch.tensor(data[\"features\"]) with torch.no_grad(): prediction = model(input_tensor) return {\"prediction\": prediction.tolist()} gRPC for Low Latency import grpc from concurrent import futures import model_pb2 import model_pb2_grpc class ModelService(model_pb2_grpc.ModelServicer): def Predict(self, request, context): features = np.array(request.features) prediction = self.model.predict(features) return model_pb2.PredictionResponse( predictions=prediction.tolist() ) Edge Deployment Model quantization ONNX conversion TensorFlow Lite Core ML for iOS Week 12: Infrastructure as Code Terraform for ML Infrastructure # GPU-enabled training cluster resource \"aws_eks_node_group\" \"gpu_nodes\" { cluster_name = aws_eks_cluster.ml_cluster.name node_group_name = \"gpu-nodes\" instance_types = [\"p3.2xlarge\"] scaling_config { desired_size = 2 max_size = 10 min_size = 1 } labels = { workload = \"ml-training\" gpu = \"true\" } } # Model storage resource \"aws_s3_bucket\" \"model_artifacts\" { bucket = \"ml-model-artifacts\" versioning { enabled = true } lifecycle_rule { enabled = true transition { days = 30 storage_class = \"STANDARD_IA\" } } } Cost Optimization Spot instances for training Auto-scaling policies Resource tagging Cost monitoring dashboards Hands-on Projects Project 1: Production ML Service Build a complete ML service with:\nContainerized model server Kubernetes deployment Auto-scaling based on metrics Blue-green deployment Monitoring and logging Project 2: Multi-Cloud ML Platform Design infrastructure supporting:\nTraining on AWS/GCP Model registry on S3 Serving on edge devices Cost optimization Performance Optimization Model Optimization Techniques Quantization (INT8, FP16) Pruning unused weights Knowledge distillation Batch inference Infrastructure Optimization GPU utilization monitoring Memory optimization Network bandwidth management Caching strategies Assessment Practical Tasks Containerize an ML application with GPU support Deploy model on Kubernetes with auto-scaling Implement A/B testing for models Create IaC for ML infrastructure Optimize model serving latency Performance Metrics Model serving latency \u003c 100ms 99.9% availability SLA Cost per inference \u003c $0.001 GPU utilization \u003e 80% Tools \u0026 Resources Essential Tools Containers: Docker, Podman Orchestration: Kubernetes, EKS, GKE IaC: Terraform, Pulumi Serving: TF Serving, TorchServe, Triton Monitoring: Prometheus, Grafana Learning Resources Kubernetes Patterns for ML NVIDIA Triton Documentation ML Infrastructure Best Practices ","week-10-kubernetes-for-ml#Week 10: Kubernetes for ML":"","week-11-model-serving-architectures#Week 11: Model Serving Architectures":"","week-12-infrastructure-as-code#Week 12: Infrastructure as Code":"","week-9-containerization-for-ml#Week 9: Containerization for ML":""},"title":"Module 3 - Infrastructure \u0026 Deployment"},"/learn-ai/docs/mlops/":{"data":{"assessment-criteria#Assessment Criteria":"","learning-objectives#Learning Objectives":"","mlops-core-module#MLOps Core Module":"","next-module#Next Module":"Continue to Module 3: Infrastructure \u0026 Deployment to learn about ML-specific infrastructure patterns and scalable deployment strategies.","practical-projects#Practical Projects":"","resources#Resources":"Documentation DVC Documentation MLflow Guide Kubeflow Pipelines Books \u0026 Articles ‚ÄúIntroducing MLOps‚Äù by Mark Treveil ‚ÄúMLOps: Continuous delivery and automation pipelines in machine learning‚Äù Google‚Äôs ‚ÄúHidden Technical Debt in Machine Learning Systems‚Äù ","tools--technologies#Tools \u0026amp; Technologies":"MLOps Core ModuleThis module applies DevOps principles to machine learning, leveraging your existing CI/CD and automation expertise.\nLearning Objectives Implement version control for data and models Build automated ML pipelines Create robust CI/CD for ML systems Establish model governance and registry Week 5: Version Control for ML Beyond Code Versioning Data Version Control (DVC)\n# Initialize DVC dvc init dvc add data/training_set.csv git add data/training_set.csv.dvc git commit -m \"Add training data\" Experiment Tracking\nMLflow setup and integration Weights \u0026 Biases for distributed teams Comparing experiment results Hands-on Lab Create a versioned ML project with:\nCode in Git Data in DVC Experiments in MLflow Models in registry Week 6: ML Pipeline Orchestration Orchestration Platforms Apache Airflow from airflow import DAG from airflow.operators.python import PythonOperator def train_model(): # Training logic pass with DAG('ml_training_pipeline', schedule_interval='@daily') as dag: train_task = PythonOperator( task_id='train_model', python_callable=train_model ) Kubeflow Pipelines Component-based architecture Kubernetes-native execution Artifact tracking Pipeline Patterns Data validation gates Model quality checks Automated retraining triggers Multi-stage deployments Week 7: CI/CD for ML Testing Strategy Unit Tests for ML def test_feature_engineering(): data = create_test_data() features = engineer_features(data) assert features.shape[1] == expected_features assert not features.isnull().any() Integration Tests Data pipeline validation Model serving endpoints Performance benchmarks Deployment Strategies Blue-Green Deployments Canary Releases # Kubernetes canary deployment spec: replicas: 10 strategy: canary: steps: - setWeight: 10 - pause: {duration: 10m} - setWeight: 50 - pause: {duration: 10m} Shadow Deployments A/B Testing Framework Week 8: Model Registry \u0026 Governance Model Registry Implementation Centralized model storage Metadata management Version comparison Promotion workflows MLflow Model Registry import mlflow # Register model mlflow.register_model( \"runs:/run_id/model\", \"production_model\" ) # Transition stages client = mlflow.tracking.MlflowClient() client.transition_model_version_stage( name=\"production_model\", version=1, stage=\"Production\" ) Governance Framework Model approval processes Audit trails Compliance documentation Performance SLAs Practical Projects Project 1: End-to-End Pipeline Build a complete MLOps pipeline that:\nTriggers on new data Validates data quality Trains model Runs tests Deploys if metrics pass Monitors in production Project 2: A/B Testing System Implement model A/B testing with:\nTraffic splitting Metric collection Statistical significance testing Automated winner selection Assessment Criteria Technical Skills Configure DVC for data versioning Build Airflow DAG for ML pipeline Implement CI/CD with model testing Deploy model with canary release Best Practices Reproducible experiments Automated testing coverage Monitoring and alerting Documentation Tools \u0026 Technologies Core Stack Version Control: Git, DVC Experiment Tracking: MLflow, W\u0026B Orchestration: Airflow, Kubeflow CI/CD: GitHub Actions, GitLab CI, Jenkins Registry: MLflow, Seldon Core Setup Commands # Install MLOps tools pip install dvc mlflow airflow pip install pytest pytest-cov pip install seldon-core # Configure MLflow server mlflow server --host 0.0.0.0 --port 5000 # Initialize Airflow airflow db init airflow webserver --port 8080 ","week-5-version-control-for-ml#Week 5: Version Control for ML":"","week-6-ml-pipeline-orchestration#Week 6: ML Pipeline Orchestration":"","week-7-cicd-for-ml#Week 7: CI/CD for ML":"","week-8-model-registry--governance#Week 8: Model Registry \u0026amp; Governance":""},"title":"Module 2 - MLOps Core"},"/learn-ai/docs/setup/":{"data":{"cloud-platform-clis#Cloud Platform CLIs":"AWS # Install AWS CLI curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install # Configure credentials aws configure Google Cloud # Install gcloud SDK echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - sudo apt update \u0026\u0026 sudo apt install google-cloud-cli Azure # Install Azure CLI curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash az login ","core-software-installation#Core Software Installation":"1. Python Environment # Install Python 3.9+ sudo apt update sudo apt install python3.9 python3.9-venv python3-pip # Install Conda (alternative) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh # Create virtual environment python3.9 -m venv ml-env source ml-env/bin/activate 2. Essential Python Packages # Core ML libraries pip install numpy pandas scikit-learn matplotlib seaborn pip install tensorflow pytorch torchvision pip install xgboost lightgbm # MLOps tools pip install mlflow dvc wandb pip install prefect airflow pip install pytest black flake8 # Serving frameworks pip install fastapi uvicorn gunicorn pip install streamlit gradio 3. Docker Setup # Install Docker curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh # Add user to docker group sudo usermod -aG docker $USER # Install Docker Compose sudo curl -L \"https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 4. Kubernetes Tools # Install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # Install minikube for local development curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube # Install Helm curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash ","development-environment-setup#Development Environment Setup":"Development Environment SetupComplete guide to set up your ML engineering development environment.","gpu-setup-optional#GPU Setup (Optional)":"NVIDIA CUDA Installation # Check GPU lspci | grep -i nvidia # Install CUDA Toolkit wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb sudo dpkg -i cuda-keyring_1.0-1_all.deb sudo apt update sudo apt install cuda # Install cuDNN # Download from NVIDIA website and install # Verify installation nvidia-smi nvcc --version PyTorch with CUDA # Install PyTorch with CUDA support pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 ","ide-configuration#IDE Configuration":"VS Code Extensions { \"recommendations\": [ \"ms-python.python\", \"ms-toolsai.jupyter\", \"ms-azuretools.vscode-docker\", \"ms-kubernetes-tools.vscode-kubernetes-tools\", \"GitHub.copilot\", \"ms-vscode-remote.remote-containers\" ] } PyCharm Setup Install PyCharm Professional Configure Python interpreter Install plugins: Docker, Kubernetes, Database Tools Set up code style and linting ","next-steps#Next Steps":" Clone the course repository Run the verification script Complete the Module 1: Fundamentals Join the community Discord/Slack channel ","project-structure-template#Project Structure Template":" ml-project/ ‚îú‚îÄ‚îÄ data/ ‚îÇ ‚îú‚îÄ‚îÄ raw/ ‚îÇ ‚îú‚îÄ‚îÄ processed/ ‚îÇ ‚îî‚îÄ‚îÄ external/ ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îú‚îÄ‚îÄ checkpoints/ ‚îÇ ‚îî‚îÄ‚îÄ production/ ‚îú‚îÄ‚îÄ notebooks/ ‚îÇ ‚îú‚îÄ‚îÄ exploration/ ‚îÇ ‚îî‚îÄ‚îÄ experiments/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ data/ ‚îÇ ‚îú‚îÄ‚îÄ features/ ‚îÇ ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îî‚îÄ‚îÄ serving/ ‚îú‚îÄ‚îÄ tests/ ‚îÇ ‚îú‚îÄ‚îÄ unit/ ‚îÇ ‚îî‚îÄ‚îÄ integration/ ‚îú‚îÄ‚îÄ configs/ ‚îÇ ‚îú‚îÄ‚îÄ training/ ‚îÇ ‚îî‚îÄ‚îÄ deployment/ ‚îú‚îÄ‚îÄ docker/ ‚îÇ ‚îú‚îÄ‚îÄ training.Dockerfile ‚îÇ ‚îî‚îÄ‚îÄ serving.Dockerfile ‚îú‚îÄ‚îÄ kubernetes/ ‚îÇ ‚îú‚îÄ‚îÄ training/ ‚îÇ ‚îî‚îÄ‚îÄ serving/ ‚îú‚îÄ‚îÄ .github/ ‚îÇ ‚îî‚îÄ‚îÄ workflows/ ‚îú‚îÄ‚îÄ requirements.txt ‚îú‚îÄ‚îÄ setup.py ‚îú‚îÄ‚îÄ Makefile ‚îî‚îÄ‚îÄ README.md ","system-requirements#System Requirements":"Hardware Minimum: 8GB RAM, 50GB storage, 4 CPU cores Recommended: 16GB RAM, 200GB SSD, 8 CPU cores GPU (optional): NVIDIA GPU with CUDA support for deep learning Operating System Ubuntu 20.04+ / macOS 11+ / Windows 11 with WSL2 Docker Desktop installed Kubernetes (minikube or kind for local development) ","troubleshooting#Troubleshooting":"Common Issues Permission Denied for Docker\nsudo usermod -aG docker $USER newgrp docker Python Package Conflicts\n# Use virtual environments python -m venv fresh-env source fresh-env/bin/activate CUDA Version Mismatch\nCheck compatibility matrix for PyTorch/TensorFlow Use Docker containers with pre-configured CUDA ","verification-script#Verification Script":"Create a script to verify your setup:\n#!/usr/bin/env python3 \"\"\"Verify ML development environment setup.\"\"\" import subprocess import sys def check_command(cmd, name): try: result = subprocess.run(cmd, shell=True, capture_output=True, text=True) if result.returncode == 0: print(f\"‚úì {name} is installed\") return True except: pass print(f\"‚úó {name} is not installed\") return False def main(): checks = [ (\"python3 --version\", \"Python 3\"), (\"docker --version\", \"Docker\"), (\"kubectl version --client\", \"kubectl\"), (\"git --version\", \"Git\"), (\"aws --version\", \"AWS CLI\"), ] print(\"Checking environment setup...\\n\") all_good = all(check_command(cmd, name) for cmd, name in checks) print(\"\\n\" + \"=\"*40) if all_good: print(\"‚úì All checks passed! Environment is ready.\") else: print(\"‚úó Some components are missing. Please install them.\") sys.exit(1) if __name__ == \"__main__\": main() "},"title":"Development Environment Setup"},"/learn-ai/projects/":{"data":{"advanced-projects#Advanced Projects":"5. Multi-Model Serving Platform Objective: Build a platform to serve multiple ML models with A/B testing\nSkills Practiced:\nModel registry Load balancing A/B testing Performance monitoring Tech Stack: FastAPI, Redis, Kubernetes, Prometheus\n6. Federated Learning System Objective: Implement privacy-preserving distributed model training\nSkills Practiced:\nDistributed systems Privacy techniques Model aggregation Security practices Tech Stack: PyTorch, gRPC, Docker, Kubernetes","beginner-projects#Beginner Projects":"1. Anomaly Detection System Objective: Build a system to detect anomalies in server metrics\nSkills Practiced:\nTime series analysis Unsupervised learning Real-time processing Alert generation Tech Stack: Python, Scikit-learn, Prometheus, Grafana\n2. Log Classification Pipeline Objective: Automatically categorize and route system logs\nSkills Practiced:\nText processing Classification algorithms Stream processing Pipeline orchestration Tech Stack: Python, Kafka, Elasticsearch, Airflow","capstone-project#Capstone Project":"End-to-End ML Platform Build a complete ML platform that includes:\nPhase 1: Data Pipeline Ingest data from multiple sources Implement data validation Create feature store Set up data versioning Phase 2: Training Pipeline Automated model training Hyperparameter tuning Experiment tracking Model registry Phase 3: Deployment \u0026 Serving Containerized deployment Auto-scaling based on load Model versioning Rollback capabilities Phase 4: Monitoring \u0026 Maintenance Performance monitoring Data drift detection Automated retraining Alert system Deliverables Source code repository Documentation Architecture diagrams Performance benchmarks Cost analysis Evaluation Criteria Code quality and organization System reliability Performance optimization Documentation completeness Security considerations ","hands-on-projects#Hands-on Projects":"Hands-on ProjectsPractical projects to reinforce your ML engineering skills.","intermediate-projects#Intermediate Projects":"3. Predictive Auto-scaling Objective: ML-based auto-scaling for Kubernetes workloads\nSkills Practiced:\nTime series forecasting Infrastructure automation Model deployment Performance optimization Tech Stack: Python, Kubernetes, Prometheus, ARIMA/LSTM\n4. CI/CD Pipeline for ML Objective: Complete MLOps pipeline with automated testing and deployment\nSkills Practiced:\nVersion control (Git, DVC) Automated testing Model validation Progressive deployment Tech Stack: GitHub Actions, MLflow, Docker, Kubernetes","project-templates#Project Templates":"Basic ML Service Template project-template/ ‚îú‚îÄ‚îÄ api/ ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ main.py ‚îÇ ‚îî‚îÄ‚îÄ models.py ‚îú‚îÄ‚îÄ ml/ ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ preprocessing.py ‚îÇ ‚îú‚îÄ‚îÄ training.py ‚îÇ ‚îî‚îÄ‚îÄ inference.py ‚îú‚îÄ‚îÄ tests/ ‚îÇ ‚îú‚îÄ‚îÄ test_api.py ‚îÇ ‚îî‚îÄ‚îÄ test_ml.py ‚îú‚îÄ‚îÄ docker/ ‚îÇ ‚îî‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ k8s/ ‚îÇ ‚îú‚îÄ‚îÄ deployment.yaml ‚îÇ ‚îî‚îÄ‚îÄ service.yaml ‚îú‚îÄ‚îÄ .github/ ‚îÇ ‚îî‚îÄ‚îÄ workflows/ ‚îÇ ‚îî‚îÄ‚îÄ ci.yml ‚îú‚îÄ‚îÄ requirements.txt ‚îî‚îÄ‚îÄ README.md ","resources-for-projects#Resources for Projects":"Datasets Kaggle Competitions Google Cloud Public Datasets AWS Open Data Compute Resources Google Colab (Free GPU) Kaggle Kernels (Free GPU) AWS Free Tier Azure Free Account Example Implementations MLOps Examples Kubeflow Examples TFX Examples ","submission-guidelines#Submission Guidelines":"Code Requirements Clean, documented code Unit tests with \u003e80% coverage Integration tests Performance benchmarks Documentation README with setup instructions Architecture documentation API documentation Deployment guide Presentation Problem statement Solution approach Technical challenges Results and metrics Future improvements "},"title":"Projects"},"/learn-ai/resources/":{"data":{"blogs--newsletters#Blogs \u0026amp; Newsletters":"","books#Books":"","certification-paths#Certification Paths":"Cloud Certifications AWS Certified Machine Learning - Specialty Google Cloud Professional ML Engineer Azure AI Engineer Associate Databricks Machine Learning Professional Vendor-Neutral Certified Kubernetes Administrator (CKA) Linux Foundation MLOps Certifications TensorFlow Developer Certificate ","community--forums#Community \u0026amp; Forums":"","conferences--events#Conferences \u0026amp; Events":"Learning ResourcesCurated collection of resources for ML Engineering learning journey.\nBooks Essential Reading ‚ÄúDesigning Machine Learning Systems‚Äù by Chip Huyen ‚ÄúMachine Learning Engineering‚Äù by Andriy Burkov ‚ÄúPractical MLOps‚Äù by Noah Gift \u0026 Alfredo Deza ‚ÄúBuilding Machine Learning Powered Applications‚Äù by Emmanuel Ameisen Advanced Topics ‚ÄúDeep Learning‚Äù by Ian Goodfellow, Yoshua Bengio, and Aaron Courville ‚ÄúPattern Recognition and Machine Learning‚Äù by Christopher Bishop ‚ÄúThe Elements of Statistical Learning‚Äù by Hastie, Tibshirani, Friedman (Free PDF) Online Courses Free Courses Fast.ai Practical Deep Learning Andrew Ng‚Äôs Machine Learning Course Google Machine Learning Crash Course Cloud Provider Courses AWS ML Learning Path Google Cloud ML Engineer Path Azure AI Fundamentals Tools \u0026 Frameworks ML Frameworks TensorFlow PyTorch Scikit-learn XGBoost MLOps Tools MLflow DVC Kubeflow Weights \u0026 Biases Serving Frameworks TensorFlow Serving TorchServe Triton Inference Server Seldon Core Community \u0026 Forums r/MachineLearning - Reddit‚Äôs ML community r/MLOps - MLOps discussions MLOps Community - Slack community for MLOps practitioners Papers with Code - ML papers with implementation Kaggle - Competitions and datasets Hugging Face Community - Discord for NLP/ML Fast.ai Forums - Deep learning discussions Stack Overflow ML - Q\u0026A platform Blogs \u0026 Newsletters Must-Follow Blogs Google AI Blog OpenAI Blog Distill.pub Neptune.ai Blog Towards Data Science Machine Learning Mastery Newsletters The Batch by Andrew Ng Import AI by Jack Clark The Gradient Papers with Code Newsletter MLOps Community Newsletter Podcasts TWIML AI Podcast - This Week in Machine Learning \u0026 AI Practical AI - Making AI practical, productive, and accessible Data Skeptic - Data science, statistics, and ML Lex Fridman Podcast - AI, science, and technology conversations The Machine Learning Podcast - ML engineering and operations YouTube Channels 3Blue1Brown - Visual math and neural networks Two Minute Papers - Latest AI research explained Sentdex - Python and ML tutorials Yannic Kilcher - Paper reviews and explanations StatQuest - Statistics and ML concepts Andrej Karpathy - Deep learning from first principles DeepLearningAI - Andrew Ng‚Äôs channel MIT OpenCourseWare - Full MIT courses Research Papers Foundational Papers ‚ÄúAttention Is All You Need‚Äù (Transformers - Vaswani et al., 2017) ‚ÄúImageNet Classification with Deep CNNs‚Äù (AlexNet - Krizhevsky et al., 2012) ‚ÄúPlaying Atari with Deep RL‚Äù (DQN - Mnih et al., 2013) ‚ÄúGenerative Adversarial Networks‚Äù (GANs - Goodfellow et al., 2014) ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers‚Äù (Devlin et al., 2018) MLOps Papers ‚ÄúHidden Technical Debt in ML Systems‚Äù (Google - Sculley et al., 2015) ‚ÄúMachine Learning: The High Interest Credit Card of Technical Debt‚Äù (Google, 2014) ‚ÄúChallenges in Deploying ML: A Survey of Case Studies‚Äù (Paleyes et al., 2020) ‚ÄúMLOps: A Systematic Literature Review‚Äù (Treveil et al., 2020) Hands-on Platforms Google Colab - Free GPU/TPU Kaggle Kernels - Competitions and datasets AWS SageMaker Studio Lab - Free ML development Paperspace Gradient - Cloud notebooks Datasets General Purpose UCI ML Repository Kaggle Datasets Google Dataset Search Specialized ImageNet - Computer Vision Common Crawl - Web data OpenML - ML datasets repository Conferences \u0026 Events Major Conferences NeurIPS (Neural Information Processing Systems) ICML (International Conference on Machine Learning) CVPR (Computer Vision and Pattern Recognition) MLOps World ICLR (International Conference on Learning Representations) KDD (Knowledge Discovery and Data Mining) Online Events MLOps Community Meetups PyTorch Developer Day TensorFlow Dev Summit AI Camp - Regular online workshops ","datasets#Datasets":"","hands-on-platforms#Hands-on Platforms":"","learning-resources#Learning Resources":"","online-courses#Online Courses":"","podcasts#Podcasts":"","research-papers#Research Papers":"","tools--frameworks#Tools \u0026amp; Frameworks":"","youtube-channels#YouTube Channels":""},"title":"Resources"}}